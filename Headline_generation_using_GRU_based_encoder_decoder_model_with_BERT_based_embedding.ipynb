{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Headline_generation_using_GRU_based_encoder_decoder_model_with_BERT_based_embedding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "40310fd2379648bb8740b289c0f6298f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ddd2245915e74089b965df673b425a1b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fd5a85062e5e4f9f8b7275b04396b5b0",
              "IPY_MODEL_db932eada4cd4f69b4a308a49928211b"
            ]
          }
        },
        "ddd2245915e74089b965df673b425a1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fd5a85062e5e4f9f8b7275b04396b5b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1b2b733d7f1742fda77be101b2b759e9",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_210b5fdd546147bb81c8b779c9254d56"
          }
        },
        "db932eada4cd4f69b4a308a49928211b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c283e2eef51247adb75256789881ede0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 293kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ea7f77d0e3b54bab9903623ae6239849"
          }
        },
        "1b2b733d7f1742fda77be101b2b759e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "210b5fdd546147bb81c8b779c9254d56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c283e2eef51247adb75256789881ede0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ea7f77d0e3b54bab9903623ae6239849": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0210fccb10ec4b2b88fa496661bc330c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a8145bb1270a4dd2828cfbe9ac9ce964",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_34c431197d924ca29b45c7ed7cd4029e",
              "IPY_MODEL_b8d4afaa227d46419a514afa2a7e2385"
            ]
          }
        },
        "a8145bb1270a4dd2828cfbe9ac9ce964": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "34c431197d924ca29b45c7ed7cd4029e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e55c91c401284fb9ba7d93dda72cb4d6",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 28,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 28,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fe9429f0f39a47378879dee1afb1a828"
          }
        },
        "b8d4afaa227d46419a514afa2a7e2385": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b563762222414077a8d16dff862a4a75",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 28.0/28.0 [00:00&lt;00:00, 98.8B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8a4a1690c3474e8a8b2a13915fec4300"
          }
        },
        "e55c91c401284fb9ba7d93dda72cb4d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fe9429f0f39a47378879dee1afb1a828": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b563762222414077a8d16dff862a4a75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8a4a1690c3474e8a8b2a13915fec4300": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c4cfaa713a0f4de491c125ba892785ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6f42e7ece6ac4d8cb960a7e1073f2dbd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_37769e43367d485691cd7cb0321aecd5",
              "IPY_MODEL_d015ec8916da4839ba88046dd6eab8cc"
            ]
          }
        },
        "6f42e7ece6ac4d8cb960a7e1073f2dbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "37769e43367d485691cd7cb0321aecd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f685034500fe46edbbbdba0fa2195911",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 466062,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 466062,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4c9d49d074dd466ab57809926217581b"
          }
        },
        "d015ec8916da4839ba88046dd6eab8cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ea2e8c94f62546919261dad7fc9b8c07",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 466k/466k [01:16&lt;00:00, 6.11kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6f48d8f0b901401a9c96ecfcc547f3d1"
          }
        },
        "f685034500fe46edbbbdba0fa2195911": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4c9d49d074dd466ab57809926217581b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ea2e8c94f62546919261dad7fc9b8c07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6f48d8f0b901401a9c96ecfcc547f3d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2f1fdcb66e504f248f866c955ac6a0ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_97021847f44f4c4dad8b30061e856e69",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_29626f016b9d4e5b91eff95db3ff5e16",
              "IPY_MODEL_ff030c3edc184aceaf89bd78defece74"
            ]
          }
        },
        "97021847f44f4c4dad8b30061e856e69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "29626f016b9d4e5b91eff95db3ff5e16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_882dc683b63249f9a9808dc09465360e",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 442,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 442,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8b94f7834ae24268a25e1a967d517a87"
          }
        },
        "ff030c3edc184aceaf89bd78defece74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_779e82056bac41418e7587610f6bd12a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 442/442 [00:00&lt;00:00, 1.04kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b34f18b0e5db42428ab87bb5c1c0b1f9"
          }
        },
        "882dc683b63249f9a9808dc09465360e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8b94f7834ae24268a25e1a967d517a87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "779e82056bac41418e7587610f6bd12a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b34f18b0e5db42428ab87bb5c1c0b1f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d25302becb6448258c76cefa9ae9c28e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_12b4434414cc45dda443e36211c7cf85",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_687ef9cc005746bcba486dbce4795483",
              "IPY_MODEL_76c93041d9a94c34a46859ecbc06dd9d"
            ]
          }
        },
        "12b4434414cc45dda443e36211c7cf85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "687ef9cc005746bcba486dbce4795483": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4baed6691ae5431cb9af5ab1913899c6",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 267967963,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 267967963,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cc4065add89444dba8ef38f30df3d457"
          }
        },
        "76c93041d9a94c34a46859ecbc06dd9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a0f6e1e6e90147608a6e82121a860ab0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 268M/268M [00:07&lt;00:00, 34.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c9c0c4d91a874660a4825285d8ad2aad"
          }
        },
        "4baed6691ae5431cb9af5ab1913899c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cc4065add89444dba8ef38f30df3d457": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a0f6e1e6e90147608a6e82121a860ab0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c9c0c4d91a874660a4825285d8ad2aad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1JaX2IJmIig"
      },
      "source": [
        "This Notebook provide implementation of a news headline generator model with GRU based encoder decoder architecture for a given news content.\n",
        "\n",
        "*   It uses a pre-trained BERT model for getting \n",
        "  - vector representation of word token in news summary which will be provided as input to encoder during training\n",
        "  - when teacher forcing is enabled, vector representation of word token in news headline which will be provided as input to deocder during training\n",
        "\n",
        "  NOTE:  Using BERT for getting vector representation of a word token **requires additional steps** as it requires context word token information also. it becomes tricky when we train model with batch inputs. **I have took care of these requirement**.\n",
        "\n",
        "\n",
        "*   This also has **customized** implementation of Greedy and Beam search decoing as token from all previous time step is required at current decoding time step. **It became very tricky while implementing beama search as I had to maintain all the previous timestep token for multiple partially generated sub-sequences(beam-search-node).**   For this I have modified existing implementation 'PyTorch-Beam-Search' (https://github.com/budzianowski/PyTorch-Beam-Search-Decoding).\n",
        "\n",
        "\n",
        "*   Also showcases some sample headline generation using both(beam/greedy) search based decoder using trained seq2seq model. although generated headline quality is very poor. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSNgQ1E9RKB0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "740b0950-2779-4a13-ed24-92ed53588dd7"
      },
      "source": [
        "'''\n",
        "mount google drive\n",
        "'''\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIqnpNyfCfme",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7300032c-faef-4f39-9e99-5deb8871e620"
      },
      "source": [
        "'''\n",
        "install required libraries\n",
        "'''\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/92/6153f4912b84ee1ab53ab45663d23e7cf3704161cb5ef18b0c07e207cef2/transformers-4.7.0-py3-none-any.whl (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 16.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 42.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Installing collected packages: huggingface-hub, tokenizers, sacremoses, transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBi7GQZzRLMI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a63c7bce-3c9e-4dd5-f5e2-f8dc11f8c3b5"
      },
      "source": [
        "'''\n",
        "import required packages\n",
        "'''\n",
        "import unicodedata\n",
        "import re\n",
        "import os\n",
        "import random\n",
        "import string\n",
        "import itertools\n",
        "import pickle\n",
        "import glob\n",
        "\n",
        "from queue import PriorityQueue\n",
        "import operator\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import transformers\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords  \n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0KysJpNRUL0"
      },
      "source": [
        "'''\n",
        "configuration for deterministic results with multiple run\n",
        "'''\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed) \n",
        "\n",
        "np.random.seed(seed)  \n",
        "random.seed(seed) \n",
        "\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLgzQ1ymRX-x"
      },
      "source": [
        "'''\n",
        "pandas configuration for showing complete content of record\n",
        "'''\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 2000)\n",
        "pd.set_option('display.float_format', '{:20,.2f}'.format)\n",
        "pd.set_option('display.max_colwidth', None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCAQA7AwRb_4"
      },
      "source": [
        "'''\n",
        "defining vocabulary class\n",
        "'''\n",
        "PAD_token = 0  # Used for padding short sentences\n",
        "SOS_token = 1  # Start-of-sentence token\n",
        "EOS_token = 2  # End-of-sentence token\n",
        "\n",
        "class Voc:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.trimmed = False #flag for trimming infrequent words\n",
        "        self.num_words = 3  # Count SOS, EOS, PAD\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"} #adding default word tokens\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        normalized_sentance=normalizeString(sentence)\n",
        "        for word in normalized_sentance.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addText(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.num_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.num_words] = word\n",
        "            self.num_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFUStG05DVwa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "78515a8c-8180-4182-d9b1-812854a994b2"
      },
      "source": [
        "'''\n",
        "load news data and show some sample \n",
        "'''\n",
        "data_path='/content/gdrive/My Drive/Capstone_Project/Data/News_Data/news_article_with_sim_score.df'\n",
        "article_df=pd.read_pickle(data_path)\n",
        "no_of_headlines=[len(similar_headlines) for similar_headlines in article_df['similar_headline'].tolist()]\n",
        "print('max no of similar headlines: ',max(no_of_headlines))\n",
        "print('min no of similar headlines: ',min(no_of_headlines))\n",
        "print('article_df shape:',article_df.shape)\n",
        "article_df.sample(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max no of similar headlines:  13\n",
            "min no of similar headlines:  0\n",
            "article_df shape: (3000, 9)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article_url</th>\n",
              "      <th>headline</th>\n",
              "      <th>content</th>\n",
              "      <th>author</th>\n",
              "      <th>published_date</th>\n",
              "      <th>read_more_source</th>\n",
              "      <th>similar_headline</th>\n",
              "      <th>similar_headline_url</th>\n",
              "      <th>similarity_scores</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1801</th>\n",
              "      <td>https://inshorts.com/en/news/foot-of-missing-businesswoman-who-stole-₹74cr-from-clients-found-on-aus-beach-1614333830092</td>\n",
              "      <td>Foot of missing businesswoman who stole ₹74cr from clients found on Aus beach</td>\n",
              "      <td>Australian police has said that campers have found the decomposed foot of missing businesswoman Melissa Caddick on a beach. Caddick, who allegedly stole A$13 million (over ₹74 crore) from her clients, disappeared on November 12 last year after federal police raided her home in Sydney. \"She may have taken her own life,\" police added.</td>\n",
              "      <td>None</td>\n",
              "      <td>2021-02-26T10:03:50.000Z</td>\n",
              "      <td>Daily Mail</td>\n",
              "      <td>[Melissa Caddick: Missing fraud suspect's foot found on Australian beach, Melissa Caddick: remains of missing businesswoman found months after disappearance, Melissa Caddick dead, police confirm, after campers find her foot on NSW South Coast, Remains of missing businesswoman and 'conwoman' Melissa Caddick have been found, NSW Health In Australia Orders Radiology Solution From Sectra For Enterprise Access To Images, Sexy Croc &amp;dash Entry #1372 &amp;dash Data Clustering Contest]</td>\n",
              "      <td>[https://www.bbc.com/news/world-australia-56205519, https://www.theguardian.com/australia-news/2021/feb/26/melissa-caddick-missing-financial-adviser-found-dead-months-after-disappearance, https://www.abc.net.au/news/2021-02-26/melissa-caddick-found-dead/13195242, https://www.dailymail.co.uk/news/article-9301259/Remains-missing-businesswoman-conwoman-Melissa-Caddick-found.html, https://www.medicalbuyer.co.in/nsw-health-in-australia-orders-radiology-solution-from-sectra-for-enterprise-access-to-images/, https://entry1372-dcround2.usercontent.dev/20200529/categories/en/economy.html]</td>\n",
              "      <td>[0.58, 0.51, 0.34, 0.49, 0.19, 0.16]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1190</th>\n",
              "      <td>https://inshorts.com/en/news/trumps-gab-account-compromised-as-hackers-target-platform-1614608033197</td>\n",
              "      <td>Trump's Gab account compromised as hackers target platform</td>\n",
              "      <td>Former US President Donald Trump's Gab account was compromised along with the social network Gab CEO Andrew Torba's account. Torba revealed that the platform is being attacked by hackers who had earlier targeted law enforcement officers. According to Wired, around 70 gigabytes of Gab data representing over 40 million posts has been stolen and includes passwords, group passwords and messages.</td>\n",
              "      <td>None</td>\n",
              "      <td>2021-03-01T14:13:53.000Z</td>\n",
              "      <td>Business Insider India</td>\n",
              "      <td>[Far-right social media Gab hacked, Trump's account targeted, Gab confirms it was hacked, Trump and Gab CEO accounts compromised during large-scale hack of alternative social media platform, Gab Hack Reveals Passwords And Private Messages, Hacktivists Attack Controversial Christian Conservative Social Media Site Gab, Leak 70 Gigabytes of Hacked Data Including Private Messages and Passwords, Gab: hack gives unprecedented look into platform used by far right, Gab Founder Andrew Torba Says Platform Was Hacked By Far-Left Activists : US : Christianity Daily, US Right-Wing Platform Gab Acknowledges it Was Hacked, Passwords, Private Posts Exposed in Hack of Gab Social Network]</td>\n",
              "      <td>[https://www.jpost.com/international/far-right-social-media-gab-hacked-trumps-account-targeted-660790, https://www.securitymagazine.com/articles/94733-gab-confirms-it-was-hacked, https://www.coloradopolitics.com/news/trump-and-gab-ceo-accounts-compromised-during-large-scale-hack-of-alternative-social-media-platform/article_379f06da-eb18-5226-b920-0833a591345f.html, https://www.forbes.com/sites/emmawoollacott/2021/03/02/gab-hack-reveals-passwords-and-private-posts/, https://www.cpomagazine.com/cyber-security/hacktivists-attack-controversial-christian-conservative-social-media-site-gab-leak-70-gigabytes-of-hacked-data-including-private-messages-and-passwords/, https://www.theguardian.com/world/2021/mar/11/gab-hack-neo-nazis-qanon-conspiracy-theories, http://www.christianitydaily.com/articles/11022/20210303/gab-founder-andrew-torba-says-platform-was-hacked-by-far-left-activists.htm, https://www.securityweek.com/us-right-wing-platform-gab-acknowledges-it-was-hacked, https://threatpost.com/hacktivists-gab-posts-passwords/164360/]</td>\n",
              "      <td>[0.76, 0.66, 0.86, 0.54, 0.47, 0.59, 0.43, 0.44, 0.54]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                   article_url                                                                       headline                                                                                                                                                                                                                                                                                                                                                                                                     content author            published_date        read_more_source                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         similar_headline  \\\n",
              "1801  https://inshorts.com/en/news/foot-of-missing-businesswoman-who-stole-₹74cr-from-clients-found-on-aus-beach-1614333830092  Foot of missing businesswoman who stole ₹74cr from clients found on Aus beach                                                              Australian police has said that campers have found the decomposed foot of missing businesswoman Melissa Caddick on a beach. Caddick, who allegedly stole A$13 million (over ₹74 crore) from her clients, disappeared on November 12 last year after federal police raided her home in Sydney. \"She may have taken her own life,\" police added.   None  2021-02-26T10:03:50.000Z              Daily Mail                                                                                                                                                                                                           [Melissa Caddick: Missing fraud suspect's foot found on Australian beach, Melissa Caddick: remains of missing businesswoman found months after disappearance, Melissa Caddick dead, police confirm, after campers find her foot on NSW South Coast, Remains of missing businesswoman and 'conwoman' Melissa Caddick have been found, NSW Health In Australia Orders Radiology Solution From Sectra For Enterprise Access To Images, Sexy Croc &dash Entry #1372 &dash Data Clustering Contest]   \n",
              "1190                      https://inshorts.com/en/news/trumps-gab-account-compromised-as-hackers-target-platform-1614608033197                     Trump's Gab account compromised as hackers target platform  Former US President Donald Trump's Gab account was compromised along with the social network Gab CEO Andrew Torba's account. Torba revealed that the platform is being attacked by hackers who had earlier targeted law enforcement officers. According to Wired, around 70 gigabytes of Gab data representing over 40 million posts has been stolen and includes passwords, group passwords and messages.   None  2021-03-01T14:13:53.000Z  Business Insider India  [Far-right social media Gab hacked, Trump's account targeted, Gab confirms it was hacked, Trump and Gab CEO accounts compromised during large-scale hack of alternative social media platform, Gab Hack Reveals Passwords And Private Messages, Hacktivists Attack Controversial Christian Conservative Social Media Site Gab, Leak 70 Gigabytes of Hacked Data Including Private Messages and Passwords, Gab: hack gives unprecedented look into platform used by far right, Gab Founder Andrew Torba Says Platform Was Hacked By Far-Left Activists : US : Christianity Daily, US Right-Wing Platform Gab Acknowledges it Was Hacked, Passwords, Private Posts Exposed in Hack of Gab Social Network]   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  similar_headline_url                                       similarity_scores  \n",
              "1801                                                                                                                                                                                                                                                                                                                                                                                                                                                                        [https://www.bbc.com/news/world-australia-56205519, https://www.theguardian.com/australia-news/2021/feb/26/melissa-caddick-missing-financial-adviser-found-dead-months-after-disappearance, https://www.abc.net.au/news/2021-02-26/melissa-caddick-found-dead/13195242, https://www.dailymail.co.uk/news/article-9301259/Remains-missing-businesswoman-conwoman-Melissa-Caddick-found.html, https://www.medicalbuyer.co.in/nsw-health-in-australia-orders-radiology-solution-from-sectra-for-enterprise-access-to-images/, https://entry1372-dcround2.usercontent.dev/20200529/categories/en/economy.html]                    [0.58, 0.51, 0.34, 0.49, 0.19, 0.16]  \n",
              "1190  [https://www.jpost.com/international/far-right-social-media-gab-hacked-trumps-account-targeted-660790, https://www.securitymagazine.com/articles/94733-gab-confirms-it-was-hacked, https://www.coloradopolitics.com/news/trump-and-gab-ceo-accounts-compromised-during-large-scale-hack-of-alternative-social-media-platform/article_379f06da-eb18-5226-b920-0833a591345f.html, https://www.forbes.com/sites/emmawoollacott/2021/03/02/gab-hack-reveals-passwords-and-private-posts/, https://www.cpomagazine.com/cyber-security/hacktivists-attack-controversial-christian-conservative-social-media-site-gab-leak-70-gigabytes-of-hacked-data-including-private-messages-and-passwords/, https://www.theguardian.com/world/2021/mar/11/gab-hack-neo-nazis-qanon-conspiracy-theories, http://www.christianitydaily.com/articles/11022/20210303/gab-founder-andrew-torba-says-platform-was-hacked-by-far-left-activists.htm, https://www.securityweek.com/us-right-wing-platform-gab-acknowledges-it-was-hacked, https://threatpost.com/hacktivists-gab-posts-passwords/164360/]  [0.76, 0.66, 0.86, 0.54, 0.47, 0.59, 0.43, 0.44, 0.54]  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCToaEpCDykz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e510cdf-6fb9-4dc9-c9ba-fccf792c998f"
      },
      "source": [
        "'''\n",
        "converting existing one-to-many mapping to one-to-one mapping\n",
        "'''\n",
        "contents=[]\n",
        "headlines=[]\n",
        "similarity_scores=[]\n",
        "count=0\n",
        "for index, row in article_df.iterrows():\n",
        "  contents.append(row['content'])\n",
        "  headlines.append(row['headline'])\n",
        "  similarity_scores.append(1.0)\n",
        "  for similar_headline,similarity_score in zip(row['similar_headline'],row['similarity_scores']):\n",
        "    contents.append(row['content'])\n",
        "    headlines.append(similar_headline)\n",
        "    similarity_scores.append(similarity_score)\n",
        "\n",
        "print('no of records in training dataset: ',len(headlines))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "no of records in training dataset:  23363\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "JluJFR0kV1Gl",
        "outputId": "b7a20966-1725-48ca-ae80-04605460c256"
      },
      "source": [
        "'''\n",
        "plotting similarity scrore distribution of similar headline with respect to reference headline\n",
        "'''\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.clf\n",
        "#plt.hist(similarity_scores, bins=5,  edgecolor=\"black\", color=\"blue\") #normed=True, \n",
        "counts, bins, _=plt.hist(similarity_scores, bins=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0],  edgecolor=\"black\", color=\"blue\") #normed=True, bins=[140, 150, 160, 175, 185, 200],\n",
        "plt.ylabel('News Article count')\n",
        "plt.xlabel('Similarity Score')\n",
        "for count, bin in zip(counts, bins):\n",
        "        plt.gca().text(bin + 0.05, count, str(count))  # +0.05 to center text\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbufjPmAV57M",
        "outputId": "1b47a8c4-f630-4729-81f6-4f96f5bcf880"
      },
      "source": [
        "'''\n",
        "decide threshold for min and max no-of-word-token in headline \n",
        "'''\n",
        "headline_len=[len(headline.split(\" \")) for headline in headlines]\n",
        "print('5th percentile length: ',np.quantile(headline_len, 0.05))\n",
        "print('25th percentile length: ',np.quantile(headline_len, 0.25))\n",
        "print('50th percentile length: ',np.quantile(headline_len, 0.50))\n",
        "print('75th percentile length: ',np.quantile(headline_len, 0.75))\n",
        "print('95th percentile length: ',np.quantile(headline_len, 0.95))\n",
        "print('99th percentile length: ',np.quantile(headline_len, 0.99))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5th percentile length:  3.0\n",
            "25th percentile length:  9.0\n",
            "50th percentile length:  11.0\n",
            "75th percentile length:  14.0\n",
            "95th percentile length:  19.0\n",
            "99th percentile length:  23.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFB56dlYD4mb"
      },
      "source": [
        "'''\n",
        "filtering news record based on similarity score and no-of-word-token in the headline\n",
        "'''\n",
        "min_word_token_in_headline=5\n",
        "max_word_token_in_headline=16\n",
        "min_similarity_score=0.5\n",
        "summaries=[]\n",
        "art_headlines=[]\n",
        "for content,headline,similarity_score in zip(contents,headlines,similarity_scores):\n",
        "   if(similarity_score>min_similarity_score):\n",
        "     headline_len=len(headline.split(' '))\n",
        "     if(min_word_token_in_headline <= headline_len <= max_word_token_in_headline):\n",
        "      summaries.append(content)\n",
        "      art_headlines.append(headline)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWkVM3Y8Egqw",
        "outputId": "bd0d0516-c531-48c5-bf8e-3eab3b3f927d"
      },
      "source": [
        "'''\n",
        "creating summary-headline pair and then randomly shuffle them\n",
        "'''\n",
        "summary_headline_pairs=list(zip(summaries,art_headlines))\n",
        "random.shuffle(summary_headline_pairs)\n",
        "len(summary_headline_pairs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15306"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oq1J_628Eh19"
      },
      "source": [
        "'''\n",
        "get train and test dataset\n",
        "'''\n",
        "train_summary_headline_pairs=summary_headline_pairs[0:13500]\n",
        "#train_summary_headline_pairs=summary_headline_pairs[0:100]#just for faster testing if code flow is working fine\n",
        "test_summary_headline_pairs=summary_headline_pairs[13500:]\n",
        "no_of_training_records=len(train_summary_headline_pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eh02X1V8F9ue"
      },
      "source": [
        "'''\n",
        "creating training batches\n",
        "'''\n",
        "summary_train=[train_summary_headline_pair[0] for train_summary_headline_pair in train_summary_headline_pairs]\n",
        "headline_train=[train_summary_headline_pair[1] for train_summary_headline_pair in train_summary_headline_pairs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338,
          "referenced_widgets": [
            "40310fd2379648bb8740b289c0f6298f",
            "ddd2245915e74089b965df673b425a1b",
            "fd5a85062e5e4f9f8b7275b04396b5b0",
            "db932eada4cd4f69b4a308a49928211b",
            "1b2b733d7f1742fda77be101b2b759e9",
            "210b5fdd546147bb81c8b779c9254d56",
            "c283e2eef51247adb75256789881ede0",
            "ea7f77d0e3b54bab9903623ae6239849",
            "0210fccb10ec4b2b88fa496661bc330c",
            "a8145bb1270a4dd2828cfbe9ac9ce964",
            "34c431197d924ca29b45c7ed7cd4029e",
            "b8d4afaa227d46419a514afa2a7e2385",
            "e55c91c401284fb9ba7d93dda72cb4d6",
            "fe9429f0f39a47378879dee1afb1a828",
            "b563762222414077a8d16dff862a4a75",
            "8a4a1690c3474e8a8b2a13915fec4300",
            "c4cfaa713a0f4de491c125ba892785ee",
            "6f42e7ece6ac4d8cb960a7e1073f2dbd",
            "37769e43367d485691cd7cb0321aecd5",
            "d015ec8916da4839ba88046dd6eab8cc",
            "f685034500fe46edbbbdba0fa2195911",
            "4c9d49d074dd466ab57809926217581b",
            "ea2e8c94f62546919261dad7fc9b8c07",
            "6f48d8f0b901401a9c96ecfcc547f3d1",
            "2f1fdcb66e504f248f866c955ac6a0ed",
            "97021847f44f4c4dad8b30061e856e69",
            "29626f016b9d4e5b91eff95db3ff5e16",
            "ff030c3edc184aceaf89bd78defece74",
            "882dc683b63249f9a9808dc09465360e",
            "8b94f7834ae24268a25e1a967d517a87",
            "779e82056bac41418e7587610f6bd12a",
            "b34f18b0e5db42428ab87bb5c1c0b1f9",
            "d25302becb6448258c76cefa9ae9c28e",
            "12b4434414cc45dda443e36211c7cf85",
            "687ef9cc005746bcba486dbce4795483",
            "76c93041d9a94c34a46859ecbc06dd9d",
            "4baed6691ae5431cb9af5ab1913899c6",
            "cc4065add89444dba8ef38f30df3d457",
            "a0f6e1e6e90147608a6e82121a860ab0",
            "c9c0c4d91a874660a4825285d8ad2aad"
          ]
        },
        "id": "Ye-UBe-OIFhh",
        "outputId": "eabedfdd-3fdf-4baa-9c9a-2d2aa9ab4753"
      },
      "source": [
        "'''\n",
        "load Bert-Model and Tokeninzer using predefined weights\n",
        "distilbert-base-uncased' model is uncased: it does not make a difference between english and English. \n",
        "'''\n",
        "model_class, tokenizer_class, pretrained_weights = (transformers.DistilBertModel, transformers.DistilBertTokenizer, 'distilbert-base-uncased')\n",
        "\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "model = model_class.from_pretrained(pretrained_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "40310fd2379648bb8740b289c0f6298f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0210fccb10ec4b2b88fa496661bc330c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c4cfaa713a0f4de491c125ba892785ee",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f1fdcb66e504f248f866c955ac6a0ed",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d25302becb6448258c76cefa9ae9c28e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=267967963.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdde85tEvOb8",
        "outputId": "a3c232f4-83c5-4926-89af-614660f3c812"
      },
      "source": [
        "'''\n",
        "initialize BOS and EOS token\n",
        "'''\n",
        "tokenizer.bos_token = tokenizer.cls_token\n",
        "tokenizer.eos_token = tokenizer.sep_token\n",
        "print('SOS token id: ',tokenizer.bos_token_id)\n",
        "print('EOS token id: ',tokenizer.eos_token_id)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SOS token id:  101\n",
            "EOS token id:  102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-s3yBiOtDSh"
      },
      "source": [
        "'''\n",
        "tokenize summary and headline\n",
        "'''\n",
        "tokenized_summaries=[tokenizer.encode(summary, add_special_tokens=True) for summary in summaries]\n",
        "tokenized_headlines=[tokenizer.encode(headline, add_special_tokens=True) for headline in art_headlines]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jp4snkTbuBwM",
        "outputId": "5e8f3eab-d5ae-43b6-f39f-3f3f49bc1c06"
      },
      "source": [
        "'''\n",
        "get maximum encoder and decoder length\n",
        "'''\n",
        "max_encoder_len=max([len(tokenized_summary) for tokenized_summary in tokenized_summaries] )\n",
        "max_decoder_len=max([len(tokenized_headline) for tokenized_headline in tokenized_headlines] )\n",
        "(max_encoder_len,max_decoder_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(125, 40)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1GV3gXDFMYw"
      },
      "source": [
        "'''\n",
        "function for providing emebded representation of BOS token at first timestep of decoding for complete batch\n",
        "'''\n",
        "def get_initial_decoder_ip(batch_size):\n",
        "  sos_token_tensor=torch.tensor([[tokenizer.bos_token_id]])\n",
        "  with torch.no_grad():\n",
        "    last_hidden_states = model(sos_token_tensor)\n",
        "  SOS_token_bert_vector=last_hidden_states[0]\n",
        "  SOS_token_bert_vector=torch.squeeze(SOS_token_bert_vector, 0)\n",
        "  decoder_input = torch.tensor([SOS_token_bert_vector.numpy() for _ in range(batch_size)])\n",
        "  print('decoder_input shape: ',decoder_input.shape)\n",
        "  decoder_input=decoder_input.permute(1,0,2)\n",
        "  print('decoder_input shape: ',decoder_input.shape)\n",
        "  return decoder_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYSeIz-n9Scg",
        "outputId": "0a8ce5b1-c0ab-459a-9efb-9b8ef9002e92"
      },
      "source": [
        "'''\n",
        "just for testing\n",
        "'''\n",
        "get_initial_decoder_ip(32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "decoder_input shape:  torch.Size([32, 1, 768])\n",
            "decoder_input shape:  torch.Size([1, 32, 768])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.5072,  0.1282, -0.2608,  ...,  0.3472, -1.0410,  0.1384],\n",
              "         [ 0.5072,  0.1282, -0.2608,  ...,  0.3472, -1.0410,  0.1384],\n",
              "         [ 0.5072,  0.1282, -0.2608,  ...,  0.3472, -1.0410,  0.1384],\n",
              "         ...,\n",
              "         [ 0.5072,  0.1282, -0.2608,  ...,  0.3472, -1.0410,  0.1384],\n",
              "         [ 0.5072,  0.1282, -0.2608,  ...,  0.3472, -1.0410,  0.1384],\n",
              "         [ 0.5072,  0.1282, -0.2608,  ...,  0.3472, -1.0410,  0.1384]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F2lX5kE9-95"
      },
      "source": [
        "'''\n",
        "tokenize news summary and headline\n",
        "'''\n",
        "encoder_max_length=512 \n",
        "decoder_max_length=40 \n",
        "inputs = tokenizer(summary_train, padding=\"max_length\", truncation=True, max_length=max_encoder_len)\n",
        "outputs = tokenizer(headline_train, padding=\"max_length\", truncation=True, max_length=max_decoder_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPsMIwqfGDLl"
      },
      "source": [
        "'''\n",
        "instance of class Batch_Data represent input to the encoder decoder model for a batch\n",
        "'''\n",
        "class Batch_Data:\n",
        "  def __init__(self, batch_ip_vector, batch_ip_length, batch_op_vector, batch_op_token_idxs, batch_mask):\n",
        "    self.batch_ip_vector=batch_ip_vector\n",
        "    self.batch_ip_length=batch_ip_length\n",
        "    self.batch_op_vector=batch_op_vector\n",
        "    self.batch_op_token_idxs=batch_op_token_idxs\n",
        "    self.batch_mask=batch_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yz7g0wHPGNLo"
      },
      "source": [
        "'''\n",
        "run this cell only once.\n",
        "compute BERT based representation of news summary and headline and store it on drive.\n",
        "this will help in faster training, as we don't have to get bert based vector representation of headline and summary during every training iteration. \n",
        "'''\n",
        "batch_start=0\n",
        "batch_end=0\n",
        "end=len(inputs.input_ids)#total no of records 13000\n",
        "batch_size=32\n",
        "summary_vector=None\n",
        "\n",
        "inputs_input_ids = np.array(inputs.input_ids)\n",
        "inputs_attention_mask = np.array(inputs.attention_mask)\n",
        "outputs_input_ids = np.array(outputs.input_ids)\n",
        "outputs_attention_mask = np.array(outputs.attention_mask)\n",
        "\n",
        "while batch_end<end:\n",
        "  batch_end=batch_start+batch_size\n",
        "  if batch_end<end:\n",
        "    pass #do nothing\n",
        "  else:\n",
        "    batch_end=end\n",
        "  print('batch_start: ',batch_start,' batch_end: ',batch_end)\n",
        "  summary_batch=inputs_input_ids[batch_start:batch_end,:]\n",
        "  summary_length_batch=[np.count_nonzero(summary==0) for summary in summary_batch]\n",
        "  headline_batch=outputs_input_ids[batch_start:batch_end,:]\n",
        "  #print('summary_batch: ',summary_batch.shape)\n",
        "  #print('headline_batch: ',headline_batch.shape)\n",
        "  attention_mask_summary = inputs_attention_mask[batch_start:batch_end,:]\n",
        "  attention_mask_headline = outputs_attention_mask[batch_start:batch_end,:]\n",
        "  #print('attention_mask_summary: ',attention_mask_summary.shape)\n",
        "  #print('attention_mask_headline: ',attention_mask_headline.shape)\n",
        "  attention_mask_summary_t = torch.tensor(attention_mask_summary)\n",
        "  attention_mask_headline_t = torch.BoolTensor(attention_mask_headline)\n",
        "  summary_batch_t = torch.tensor(summary_batch) \n",
        "  headline_batch_t = torch.tensor(headline_batch)  \n",
        "  with torch.no_grad():\n",
        "    last_hidden_states = model(summary_batch_t, attention_mask=attention_mask_summary_t)\n",
        "  summary_batch_vector=last_hidden_states[0]\n",
        "  with torch.no_grad():\n",
        "    last_hidden_states = model(headline_batch_t, attention_mask=attention_mask_headline_t)\n",
        "  headline_batch_vector=last_hidden_states[0]\n",
        "\n",
        "  batch_data=Batch_Data(summary_batch_vector,summary_length_batch,headline_batch_vector,headline_batch_t,attention_mask_headline_t)\n",
        "  '''print('batch_ip_vector: ',batch_data.batch_ip_vector.shape, \"type: \",type(batch_data.batch_ip_vector))\n",
        "  print('batch_op_vector: ',batch_data.batch_op_vector.shape, \"type: \",type(batch_data.batch_op_vector))\n",
        "  print('batch_op_token_idxs: ',batch_data.batch_op_token_idxs.shape, \"type: \",type(batch_data.batch_op_token_idxs))\n",
        "  print('batch_mask: ',batch_data.batch_mask.shape, \"type: \",type(batch_data.batch_mask))\n",
        "  print('batch_ip_length: ',len(batch_data.batch_ip_length), \"type: \",type(batch_data.batch_ip_length))'''\n",
        "  batch_file_path='/content/gdrive/My Drive/Capstone_Project/Data/Bert_vectors/batch_data/batch_'+str(batch_start)+'_'+str(batch_end)+'.pickle'\n",
        "  with open(batch_file_path, 'wb') as file_handle:\n",
        "    pickle.dump(batch_data, file_handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "  batch_start=batch_end"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxAmnkGEFqep"
      },
      "source": [
        "'''\n",
        "utility function for creating batche from stored BERT based news headline and summary vector \n",
        "'''\n",
        "def prepare_batches():\n",
        "  training_batch_location='/content/gdrive/My Drive/Capstone_Project/Data/Bert_vectors/batch_data/*'\n",
        "  batch_files=glob.glob(training_batch_location)\n",
        "  random.shuffle(batch_files)\n",
        "  #print(batch_files)\n",
        "  return batch_files\n",
        "\n",
        "def get_data_for_current_iteration(iteration_index,batch_files,batch_size=32):\n",
        "  no_of_files_per_batch=int(batch_size/32) #every batch file has 32 training records and batch size shalll be multiple of 32\n",
        "  start_idx=iteration_index*no_of_files_per_batch\n",
        "  end_idx=start_idx+no_of_files_per_batch\n",
        "  files_for_this_iter=batch_files[start_idx:end_idx]#end_idx is exclusive and start_idx is inclusive\n",
        "  print('files_for_this_iter: ',files_for_this_iter)\n",
        "  batch_data=None\n",
        "  for file in files_for_this_iter:\n",
        "    #print('for loop')\n",
        "    with open(file, 'rb') as file_handle:\n",
        "      batch_data_loaded=pickle.load(file_handle)  \n",
        "      #print('*************1')\n",
        "    if (batch_data==None):\n",
        "      #print('*************2')\n",
        "      batch_data=batch_data_loaded\n",
        "      #print('*************3')\n",
        "    else:\n",
        "      #print('*************4')\n",
        "      batch_ip_vector=torch.vstack((batch_data.batch_ip_vector,batch_data_loaded.batch_ip_vector))\n",
        "      batch_ip_length=batch_data.batch_ip_length + batch_data_loaded.batch_ip_length\n",
        "      batch_op_vector=torch.vstack((batch_data.batch_op_vector,batch_data_loaded.batch_op_vector))\n",
        "      batch_op_token_idxs=torch.vstack((batch_data.batch_op_token_idxs,batch_data_loaded.batch_op_token_idxs))\n",
        "      batch_mask=torch.vstack((batch_data.batch_mask,batch_data_loaded.batch_mask))\n",
        "      batch_data=Batch_Data(batch_ip_vector,batch_ip_length,batch_op_vector,batch_op_token_idxs,batch_mask)\n",
        "  #print('*************5')\n",
        "  return batch_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qpi3wxpsPQEj"
      },
      "source": [
        "'''\n",
        "test function for ip data generation for a given training iteration\n",
        "'''\n",
        "def trainIters():\n",
        "  num_sample=128 #TODO need to be initialized prperly\n",
        "  batch_size=64 #TODO need to be initialized prperly\n",
        "  n_epoch=2 #TODO need to be initialized prperly\n",
        "  max_target_len=44 #TODO need to be initialized prperly\n",
        "  num_iteration=int(num_sample/batch_size)\n",
        "  for epoch in range(n_epoch):\n",
        "    print('epoch is in progress: ',epoch+1)  \n",
        "    batch_files=prepare_batches()\n",
        "    training_batches =[] \n",
        "    for iteration_index in range(num_iteration):\n",
        "      print('*********************iteration index:',str(iteration_index),'*********************')\n",
        "      # Run a training iteration with batch\n",
        "      # Extract fields from batch\n",
        "      batch_data=get_data_for_current_iteration(iteration_index,batch_files,batch_size)\n",
        "      input_variable,lengths,target_variable,batch_op_token_idxs,mask =batch_data.batch_ip_vector,batch_data.batch_ip_length,batch_data.batch_op_vector,batch_data.batch_op_token_idxs,batch_data.batch_mask\n",
        "      input_variable=input_variable.permute(1,0,2)\n",
        "      input_variable=input_variable.permute(1,0,2)\n",
        "      lengths = torch.tensor(lengths)\n",
        "      print('input_variable: ',input_variable.shape, \"type: \",type(input_variable))\n",
        "      print('target_variable: ',target_variable.shape, \"type: \",type(target_variable))\n",
        "      print('mask: ',mask.shape, \"type: \",type(mask))\n",
        "      print('batch_ip_length: ',lengths.shape, \"type: \",type(lengths))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1o_357dBX2x",
        "outputId": "736b7256-4bb4-4cb2-8ba9-b434c9564331"
      },
      "source": [
        "'''\n",
        "test ip data generation for a given training iteration\n",
        "'''\n",
        "trainIters()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch is in progress:  1\n",
            "*********************iteration index: 0 *********************\n",
            "files_for_this_iter:  ['/content/gdrive/My Drive/Capstone_Project/Data/Bert_vectors/batch_data/batch_5888_5920.pickle', '/content/gdrive/My Drive/Capstone_Project/Data/Bert_vectors/batch_data/batch_9216_9248.pickle']\n",
            "input_variable:  torch.Size([64, 125, 768]) type:  <class 'torch.Tensor'>\n",
            "target_variable:  torch.Size([64, 40, 768]) type:  <class 'torch.Tensor'>\n",
            "mask:  torch.Size([64, 40]) type:  <class 'torch.Tensor'>\n",
            "batch_ip_length:  torch.Size([64]) type:  <class 'torch.Tensor'>\n",
            "*********************iteration index: 1 *********************\n",
            "files_for_this_iter:  ['/content/gdrive/My Drive/Capstone_Project/Data/Bert_vectors/batch_data/batch_12768_12800.pickle', '/content/gdrive/My Drive/Capstone_Project/Data/Bert_vectors/batch_data/batch_2304_2336.pickle']\n",
            "input_variable:  torch.Size([64, 125, 768]) type:  <class 'torch.Tensor'>\n",
            "target_variable:  torch.Size([64, 40, 768]) type:  <class 'torch.Tensor'>\n",
            "mask:  torch.Size([64, 40]) type:  <class 'torch.Tensor'>\n",
            "batch_ip_length:  torch.Size([64]) type:  <class 'torch.Tensor'>\n",
            "epoch is in progress:  2\n",
            "*********************iteration index: 0 *********************\n",
            "files_for_this_iter:  ['/content/gdrive/My Drive/Capstone_Project/Data/Bert_vectors/batch_data/batch_12448_12480.pickle', '/content/gdrive/My Drive/Capstone_Project/Data/Bert_vectors/batch_data/batch_6752_6784.pickle']\n",
            "input_variable:  torch.Size([64, 125, 768]) type:  <class 'torch.Tensor'>\n",
            "target_variable:  torch.Size([64, 40, 768]) type:  <class 'torch.Tensor'>\n",
            "mask:  torch.Size([64, 40]) type:  <class 'torch.Tensor'>\n",
            "batch_ip_length:  torch.Size([64]) type:  <class 'torch.Tensor'>\n",
            "*********************iteration index: 1 *********************\n",
            "files_for_this_iter:  ['/content/gdrive/My Drive/Capstone_Project/Data/Bert_vectors/batch_data/batch_4064_4096.pickle', '/content/gdrive/My Drive/Capstone_Project/Data/Bert_vectors/batch_data/batch_11936_11968.pickle']\n",
            "input_variable:  torch.Size([64, 125, 768]) type:  <class 'torch.Tensor'>\n",
            "target_variable:  torch.Size([64, 40, 768]) type:  <class 'torch.Tensor'>\n",
            "mask:  torch.Size([64, 40]) type:  <class 'torch.Tensor'>\n",
            "batch_ip_length:  torch.Size([64]) type:  <class 'torch.Tensor'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zY7R9wXCElra"
      },
      "source": [
        "'''\n",
        "GRU based encoder class without any embedding layer (as input will precomputed bert vector representation of news data)\n",
        "'''\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, embbed_dim, hidden_dim, num_layers):\n",
        "       super(Encoder, self).__init__()\n",
        "       #set the encoder input dimesion , embbed dimesion, hidden dimesion, and number of layers \n",
        "       self.hidden_dim = hidden_dim\n",
        "       self.num_layers = num_layers\n",
        "       self.embbed_dim=embbed_dim\n",
        "       #intialize the GRU to take the input dimetion of embbed, and output dimention of hidden and\n",
        "       #set the number of gru layers\n",
        "       self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers)\n",
        "\n",
        "  def forward(self, input_seq, input_lengths, hidden=None):\n",
        "    print('inside encoder forward function || input_seq shape: ',input_seq.shape )\n",
        "    print('inside encoder forward function || input_lengths shape: ',input_lengths.shape )\n",
        "    if(hidden!=None):\n",
        "      torch.set_printoptions(threshold=10000)\n",
        "      print('inside encoder forward function || hidden shape: ',hidden )\n",
        "    # Pack padded batch of sequences for RNN module\n",
        "    \n",
        "    # Forward pass through GRU\n",
        "    outputs, hidden = self.gru(input_seq, hidden)\n",
        "    '''\n",
        "    packed = nn.utils.rnn.pack_padded_sequence(input_seq, input_lengths)\n",
        "    outputs, hidden = self.gru(packed, hidden)##TODO \n",
        "    outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "    '''\n",
        "    # Unpack padding\n",
        "    \n",
        "    #print('inside encoder forward function || outputs shape: ',outputs.shape )\n",
        "    #print('inside encoder forward function || hidden shape: ',hidden )\n",
        "    #print('inside encoder forward function || outputs[:, : ,self.hidden_dim:] shape: ',outputs[:, : ,self.hidden_dim:].shape )\n",
        "    # Sum bidirectional GRU outputs\n",
        "    #outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
        "    # Return output and final hidden state\n",
        "    print('inside encoder forward function || outputs shape: ',outputs.shape )\n",
        "    print('inside encoder forward function || hidden shape: ',hidden.shape )\n",
        "    return outputs, hidden   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaUiXOrxwhYn"
      },
      "source": [
        "'''\n",
        "Luong attention layer\n",
        "'''\n",
        "class Attn(nn.Module):\n",
        "    def __init__(self, method, hidden_size):\n",
        "        super(Attn, self).__init__()\n",
        "        self.method = method\n",
        "        if self.method not in ['dot', 'general', 'concat']:\n",
        "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
        "        self.hidden_size = hidden_size\n",
        "        if self.method == 'general':\n",
        "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
        "        elif self.method == 'concat':\n",
        "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
        "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
        "\n",
        "    def dot_score(self, hidden, encoder_output):\n",
        "        return torch.sum(hidden * encoder_output, dim=2)\n",
        "\n",
        "    def general_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(encoder_output)\n",
        "        return torch.sum(hidden * energy, dim=2)\n",
        "\n",
        "    def concat_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
        "        return torch.sum(self.v * energy, dim=2)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # Calculate the attention weights (energies) based on the given method\n",
        "        if self.method == 'general':\n",
        "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'concat':\n",
        "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'dot':\n",
        "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
        "\n",
        "        # Transpose max_length and batch_size dimensions\n",
        "        attn_energies = attn_energies.t()\n",
        "\n",
        "        # Return the softmax normalized probability scores (with added dimension)\n",
        "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amH-BDKNwjmM"
      },
      "source": [
        "'''\n",
        "GRU with Luong Attn based Decoder class \n",
        "'''\n",
        "class LuongAttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, attn_model, embbed_dim, hidden_size, output_size, num_layers=1 ):\n",
        "        super(LuongAttnDecoderRNN, self).__init__()\n",
        "\n",
        "        # Keep for reference\n",
        "        self.embbed_dim=embbed_dim\n",
        "        self.attn_model = attn_model\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Define layers\n",
        "        self.gru = nn.GRU(self.embbed_dim, self.hidden_size, self.num_layers)\n",
        "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.attn = Attn(attn_model, hidden_size)\n",
        "\n",
        "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
        "        # Note: we run this one step (word) at a time\n",
        "        #print(\"decoder forward 1 embedded shape: \",embedded.shape)\n",
        "        #embedded = self.embedding_dropout(embedded)\n",
        "        # Forward through unidirectional GRU\n",
        "        rnn_output, hidden = self.gru(input_step, last_hidden)\n",
        "        #print(\"decoder forward 2 rnn_output shape: \",rnn_output.shape)\n",
        "        #print(\"decoder forward 2 hidden shape: \",hidden.shape)\n",
        "        # Calculate attention weights from the current GRU output\n",
        "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
        "        #print(\"decoder forward 3 attn_weights shape: \",attn_weights.shape)\n",
        "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
        "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
        "        #print(\"decoder forward 4 context shape: \",context.shape)\n",
        "        # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
        "        rnn_output = rnn_output.squeeze(0)\n",
        "        context = context.squeeze(1)\n",
        "        #print(\"decoder forward 5 rnn_output.squeeze shape: \",rnn_output.shape)\n",
        "        #print(\"decoder forward 5 context.squeeze shape: \",context.shape)\n",
        "        concat_input = torch.cat((rnn_output, context), 1)\n",
        "        concat_output = torch.tanh(self.concat(concat_input))\n",
        "        #print(\"decoder forward 6 concat_input shape: \",concat_input.shape)\n",
        "        #print(\"decoder forward 6 concat_output shape: \",concat_output.shape)        \n",
        "        # Predict next word using Luong eq. 6\n",
        "        output = self.out(concat_output)\n",
        "        #print(\"decoder forward 7 output shape: \",output.shape)\n",
        "        output = F.softmax(output, dim=1)\n",
        "        #print(\"decoder forward 8 output shape: \",output.shape)\n",
        "        # {Return word2 output} {and output} {output} and final hidden state\n",
        "        return output, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XNhgm8Nwmtl"
      },
      "source": [
        "'''\n",
        "loss function that calculates the average negative log likelihood of the elements that correspond to a 1 in the mask tensor\n",
        "'''\n",
        "def maskNLLLoss(inp, target, mask):\n",
        "    nTotal = mask.sum()\n",
        "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
        "    loss = crossEntropy.masked_select(mask).mean()\n",
        "    loss = loss.to(device)\n",
        "    return loss, nTotal.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoamWCznwrzK"
      },
      "source": [
        "'''\n",
        "function for performing a single training iteration\n",
        "'''\n",
        "def train(input_variable, lengths, target_variable, target_op_token_idxs, mask, max_target_len, encoder, decoder,\n",
        "          encoder_optimizer, decoder_optimizer, batch_size, clip,decoder_ip_initial, teacher_forcing_ratio=1):\n",
        "    # Zero gradients\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    # Set device options\n",
        "    input_variable = input_variable.to(device)\n",
        "    target_variable = target_variable.to(device)\n",
        "    mask = mask.to(device)\n",
        "    # Lengths for rnn packing should always be on the cpu\n",
        "    lengths = lengths.to(\"cpu\")\n",
        "\n",
        "    # Initialize variables\n",
        "    loss = 0\n",
        "    print_losses = []\n",
        "    n_totals = 0\n",
        "\n",
        "    # Forward pass through encoder\n",
        "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
        "\n",
        "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
        "    print('inside train function decoder_ip_initial shape: ',decoder_ip_initial.shape)\n",
        "    decoder_input = decoder_ip_initial#sos token for all the training sample in a given batch\n",
        "    decoder_input = decoder_input.to(device)\n",
        "\n",
        "    # Set initial decoder hidden state to the encoder's final hidden state\n",
        "    decoder_hidden = encoder_hidden[:decoder.num_layers] #this important and handle scenario where no of layer for GRU varries in encoder decoder\n",
        "    #TODO check why 'encoder_hidden[:decoder.n_layers]'? not 'encoder_hidden[:encoder.n_layers]'\n",
        "    #print('inside function train decoder_hidden: ',decoder_hidden.shape)\n",
        "    #print('inside function train encoder_hidden[:encoder.num_layers]: ',encoder_hidden[:encoder.num_layers].shape)\n",
        "    # Determine if we are using teacher forcing this iteration\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False #TODO need to work on this\n",
        "    #print('before the decoder forward pass')\n",
        "    # Forward batch of sequences through decoder one time step at a time\n",
        "    if use_teacher_forcing:\n",
        "        #iterate through timesteps for decoder \n",
        "        print('teacher forcing will be used for this batch')\n",
        "        for timestep in range(1,max_target_len):\n",
        "            #print('before decoder forward pass: ')\n",
        "            #print('decoder_input shape: ',decoder_input.shape)\n",
        "            #print('decoder_hidden shape: ',decoder_hidden.shape)\n",
        "            #print('encoder_outputs shape: ',encoder_outputs.shape)\n",
        "            print('timestep: ',timestep,' inside train function1: ',decoder_input.shape)\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)#(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            # Teacher forcing: next input is current target\n",
        "            print('timestep: ',timestep,' inside train function2: ',decoder_input.shape)\n",
        "            decoder_input = torch.unsqueeze(target_variable[timestep],0)#[1,64] use next timestamp token from target seq  as ip to decoder at nexe time step\n",
        "            #print('target_variable[timestep] shape: ',target_variable[timestep].shape)#[64]\n",
        "            #print('target_variable[timestep].view(1, -1) shape: ',target_variable[timestep].view(1, -1).shape)#[1,64]\n",
        "            # Calculate and accumulate loss\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_op_token_idxs[timestep], mask[timestep])\n",
        "            print('timestep : ',timestep,' mask_loss: ',mask_loss,' loss: ',loss)\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "            non_padding_token_count=mask[timestep+1].sum()#non_padding_token_count for next timestep\n",
        "            #print('non_padding_token_count: ',non_padding_token_count)\n",
        "            if(non_padding_token_count==0):#all tokens are padding token for next timestep for all records in batches \n",
        "              break\n",
        "    else:\n",
        "        print('teacher forcing won\\'t be used for this batch')\n",
        "        for timestep in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            # No teacher forcing: next input is decoder's own current output\n",
        "            _, topi = decoder_output.topk(1)\n",
        "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
        "            decoder_input = decoder_input.to(device)\n",
        "            # Calculate and accumulate loss\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "\n",
        "    # Perform backpropatation\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip gradients: gradients are modified in place\n",
        "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
        "\n",
        "    # Adjust model weights\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return sum(print_losses) / n_totals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTn_prSz2r_w"
      },
      "source": [
        "'''\n",
        "function responsible for running n epoch of training given the passed models, optimizers, data etc.\n",
        "'''\n",
        "def trainIters(n_epoch, batch_size, num_sample, max_target_len, encoder, decoder, encoder_optimizer, decoder_optimizer, decoder_ip_initial, print_every, clip):\n",
        "  num_iteration=int(num_sample/batch_size)\n",
        "  num_iteration=2 #TODO need to commented just for testing if complete training code is working fine\n",
        "  for epoch in range(n_epoch):\n",
        "    print('epoch is in progress: ',epoch+1)  \n",
        "    batch_files=prepare_batches()\n",
        "    print_loss=0\n",
        "    for iteration_index in range(num_iteration):\n",
        "      print('*********************iteration index:',str(iteration_index),'*********************')\n",
        "      # Run a training iteration with batch\n",
        "      # Extract fields from batch\n",
        "      batch_data=get_data_for_current_iteration(iteration_index,batch_files,batch_size)\n",
        "      input_variable,lengths,target_variable,target_op_token_idxs,mask =batch_data.batch_ip_vector,batch_data.batch_ip_length,batch_data.batch_op_vector,batch_data.batch_op_token_idxs,batch_data.batch_mask\n",
        "      input_variable=input_variable.permute(1,0,2)\n",
        "      target_variable=target_variable.permute(1,0,2)\n",
        "      target_op_token_idxs=target_op_token_idxs.permute(1,0)\n",
        "      mask=mask.permute(1,0)\n",
        "      lengths = torch.tensor(lengths)\n",
        "      print('input_variable: ',input_variable.shape, \"type: \",type(input_variable))\n",
        "      print('target_variable: ',target_variable.shape, \"type: \",type(target_variable))\n",
        "      print('target_op_token_idxs: ',target_op_token_idxs.shape, \"type: \",type(target_op_token_idxs))\n",
        "      #torch.set_printoptions(profile=\"full\")\n",
        "      #print('target_op_token_idxs: ',target_op_token_idxs)\n",
        "      #torch.set_printoptions(profile=\"default\") # reset#\n",
        "      #break\n",
        "      print('mask: ',mask.shape, \"type: \",type(mask))\n",
        "      print('batch_ip_length: ',lengths.shape, \"type: \",type(lengths))\n",
        "      \n",
        "      print('inside trainIters before calling train')\n",
        "      loss = train(input_variable, lengths, target_variable, target_op_token_idxs, mask, max_target_len, encoder, decoder,\n",
        "          encoder_optimizer, decoder_optimizer, batch_size, clip,decoder_ip_initial, teacher_forcing_ratio=1)\n",
        "      print('inside trainIters after calling train')\n",
        "      print_loss += loss\n",
        "      print_every=1 #print after every 10 iteration\n",
        "      # Print training progress\n",
        "      if iteration_index % print_every == 0: \n",
        "        print_loss_avg = print_loss / print_every\n",
        "        print('Epoch: ',epoch+1,' Iteration: ',iteration_index,' avg_loss: ',print_loss_avg)\n",
        "        print_loss = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkyiQfdpwyxS",
        "outputId": "52995f81-6b2e-47fc-f7d3-bbc30f4356bf"
      },
      "source": [
        "'''\n",
        "Configure training and optimization parameter\n",
        "'''\n",
        "encoder_n_layers=1 \n",
        "decoder_n_layers=1\n",
        "\n",
        "clip = 50.0\n",
        "teacher_forcing_ratio = 1.0\n",
        "learning_rate = 0.0001\n",
        "decoder_learning_ratio = 5.0\n",
        "n_epoch = 1 #10\n",
        "print_every = 10\n",
        "save_every = 500\n",
        "\n",
        "##\n",
        "num_sample = 128 #padded_summaries.shape\n",
        "max_target_len = 40 #headline_max_len\n",
        "\n",
        "\n",
        "##\n",
        "vocab_len=30522 #\n",
        "output_size=30521 # BERT vocab_size  30522\n",
        "embed_size = 768\n",
        "hidden_size = 256\n",
        "batch_size = 32\n",
        "#num_iteration = 100000\n",
        "\n",
        "\n",
        "# Initialize encoder & decoder models\n",
        "encoder = Encoder(embed_size, hidden_size, encoder_n_layers)\n",
        "attn_model = 'dot'\n",
        "decoder = LuongAttnDecoderRNN(attn_model, embed_size, hidden_size, output_size, decoder_n_layers)\n",
        "\n",
        "# Ensure dropout layers are in train mode\n",
        "encoder.train()\n",
        "decoder.train()\n",
        "\n",
        "# Initialize optimizers\n",
        "print('Building optimizers ...')\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "\n",
        "decoder_ip_initial=get_initial_decoder_ip(batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building optimizers ...\n",
            "decoder_input shape:  torch.Size([32, 1, 768])\n",
            "decoder_input shape:  torch.Size([1, 32, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Qv_RLm8w2PT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddb95738-e091-4098-9e22-26f3754c0599"
      },
      "source": [
        "'''\n",
        "train the model for specified no of epoch\n",
        "'''\n",
        "trainIters(n_epoch, batch_size, num_sample, max_target_len, encoder, decoder, encoder_optimizer, decoder_optimizer, decoder_ip_initial, print_every, clip)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch is in progress:  1\n",
            "*********************iteration index: 0 *********************\n",
            "files_for_this_iter:  ['/content/gdrive/My Drive/Capstone_Project/Data/Bert_vectors/batch_data/batch_384_416.pickle']\n",
            "input_variable:  torch.Size([125, 32, 768]) type:  <class 'torch.Tensor'>\n",
            "target_variable:  torch.Size([40, 32, 768]) type:  <class 'torch.Tensor'>\n",
            "target_op_token_idxs:  torch.Size([40, 32]) type:  <class 'torch.Tensor'>\n",
            "mask:  torch.Size([40, 32]) type:  <class 'torch.Tensor'>\n",
            "batch_ip_length:  torch.Size([32]) type:  <class 'torch.Tensor'>\n",
            "inside trainIters before calling train\n",
            "inside encoder forward function || input_seq shape:  torch.Size([125, 32, 768])\n",
            "inside encoder forward function || input_lengths shape:  torch.Size([32])\n",
            "inside encoder forward function || outputs shape:  torch.Size([125, 32, 256])\n",
            "inside encoder forward function || hidden shape:  torch.Size([1, 32, 256])\n",
            "inside train function decoder_ip_initial shape:  torch.Size([1, 32, 768])\n",
            "teacher forcing will be used for this batch\n",
            "timestep:  1  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  1  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  1  mask_loss:  tensor(10.3694, grad_fn=<MeanBackward0>)  loss:  0\n",
            "timestep:  2  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  2  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  2  mask_loss:  tensor(10.3232, grad_fn=<MeanBackward0>)  loss:  tensor(10.3694, grad_fn=<AddBackward0>)\n",
            "timestep:  3  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  3  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  3  mask_loss:  tensor(10.3587, grad_fn=<MeanBackward0>)  loss:  tensor(20.6926, grad_fn=<AddBackward0>)\n",
            "timestep:  4  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  4  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  4  mask_loss:  tensor(10.3415, grad_fn=<MeanBackward0>)  loss:  tensor(31.0513, grad_fn=<AddBackward0>)\n",
            "timestep:  5  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  5  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  5  mask_loss:  tensor(10.3089, grad_fn=<MeanBackward0>)  loss:  tensor(41.3929, grad_fn=<AddBackward0>)\n",
            "timestep:  6  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  6  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  6  mask_loss:  tensor(10.3151, grad_fn=<MeanBackward0>)  loss:  tensor(51.7017, grad_fn=<AddBackward0>)\n",
            "timestep:  7  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  7  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  7  mask_loss:  tensor(10.3410, grad_fn=<MeanBackward0>)  loss:  tensor(62.0168, grad_fn=<AddBackward0>)\n",
            "timestep:  8  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  8  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  8  mask_loss:  tensor(10.3530, grad_fn=<MeanBackward0>)  loss:  tensor(72.3578, grad_fn=<AddBackward0>)\n",
            "timestep:  9  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  9  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  9  mask_loss:  tensor(10.3442, grad_fn=<MeanBackward0>)  loss:  tensor(82.7109, grad_fn=<AddBackward0>)\n",
            "timestep:  10  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  10  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  10  mask_loss:  tensor(10.3166, grad_fn=<MeanBackward0>)  loss:  tensor(93.0551, grad_fn=<AddBackward0>)\n",
            "timestep:  11  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  11  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  11  mask_loss:  tensor(10.3646, grad_fn=<MeanBackward0>)  loss:  tensor(103.3717, grad_fn=<AddBackward0>)\n",
            "timestep:  12  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  12  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  12  mask_loss:  tensor(10.3501, grad_fn=<MeanBackward0>)  loss:  tensor(113.7363, grad_fn=<AddBackward0>)\n",
            "timestep:  13  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  13  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  13  mask_loss:  tensor(10.3572, grad_fn=<MeanBackward0>)  loss:  tensor(124.0864, grad_fn=<AddBackward0>)\n",
            "timestep:  14  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  14  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  14  mask_loss:  tensor(10.3486, grad_fn=<MeanBackward0>)  loss:  tensor(134.4435, grad_fn=<AddBackward0>)\n",
            "timestep:  15  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  15  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  15  mask_loss:  tensor(10.3416, grad_fn=<MeanBackward0>)  loss:  tensor(144.7921, grad_fn=<AddBackward0>)\n",
            "timestep:  16  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  16  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  16  mask_loss:  tensor(10.3504, grad_fn=<MeanBackward0>)  loss:  tensor(155.1337, grad_fn=<AddBackward0>)\n",
            "timestep:  17  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  17  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  17  mask_loss:  tensor(10.3403, grad_fn=<MeanBackward0>)  loss:  tensor(165.4840, grad_fn=<AddBackward0>)\n",
            "timestep:  18  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  18  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  18  mask_loss:  tensor(10.3494, grad_fn=<MeanBackward0>)  loss:  tensor(175.8243, grad_fn=<AddBackward0>)\n",
            "timestep:  19  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  19  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  19  mask_loss:  tensor(10.3539, grad_fn=<MeanBackward0>)  loss:  tensor(186.1737, grad_fn=<AddBackward0>)\n",
            "timestep:  20  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  20  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  20  mask_loss:  tensor(10.3884, grad_fn=<MeanBackward0>)  loss:  tensor(196.5276, grad_fn=<AddBackward0>)\n",
            "timestep:  21  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  21  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  21  mask_loss:  tensor(10.3438, grad_fn=<MeanBackward0>)  loss:  tensor(206.9160, grad_fn=<AddBackward0>)\n",
            "timestep:  22  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  22  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  22  mask_loss:  tensor(10.3663, grad_fn=<MeanBackward0>)  loss:  tensor(217.2598, grad_fn=<AddBackward0>)\n",
            "timestep:  23  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  23  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  23  mask_loss:  tensor(10.3697, grad_fn=<MeanBackward0>)  loss:  tensor(227.6261, grad_fn=<AddBackward0>)\n",
            "timestep:  24  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  24  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  24  mask_loss:  tensor(10.4267, grad_fn=<MeanBackward0>)  loss:  tensor(237.9957, grad_fn=<AddBackward0>)\n",
            "timestep:  25  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  25  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  25  mask_loss:  tensor(10.3422, grad_fn=<MeanBackward0>)  loss:  tensor(248.4224, grad_fn=<AddBackward0>)\n",
            "timestep:  26  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  26  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  26  mask_loss:  tensor(10.2931, grad_fn=<MeanBackward0>)  loss:  tensor(258.7646, grad_fn=<AddBackward0>)\n",
            "timestep:  27  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  27  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  27  mask_loss:  tensor(10.3470, grad_fn=<MeanBackward0>)  loss:  tensor(269.0576, grad_fn=<AddBackward0>)\n",
            "inside trainIters after calling train\n",
            "Epoch:  1  Iteration:  0  avg_loss:  10.343737881316317\n",
            "*********************iteration index: 1 *********************\n",
            "files_for_this_iter:  ['/content/gdrive/My Drive/Capstone_Project/Data/Bert_vectors/batch_data/batch_6464_6496.pickle']\n",
            "input_variable:  torch.Size([125, 32, 768]) type:  <class 'torch.Tensor'>\n",
            "target_variable:  torch.Size([40, 32, 768]) type:  <class 'torch.Tensor'>\n",
            "target_op_token_idxs:  torch.Size([40, 32]) type:  <class 'torch.Tensor'>\n",
            "mask:  torch.Size([40, 32]) type:  <class 'torch.Tensor'>\n",
            "batch_ip_length:  torch.Size([32]) type:  <class 'torch.Tensor'>\n",
            "inside trainIters before calling train\n",
            "inside encoder forward function || input_seq shape:  torch.Size([125, 32, 768])\n",
            "inside encoder forward function || input_lengths shape:  torch.Size([32])\n",
            "inside encoder forward function || outputs shape:  torch.Size([125, 32, 256])\n",
            "inside encoder forward function || hidden shape:  torch.Size([1, 32, 256])\n",
            "inside train function decoder_ip_initial shape:  torch.Size([1, 32, 768])\n",
            "teacher forcing will be used for this batch\n",
            "timestep:  1  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  1  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  1  mask_loss:  tensor(10.3290, grad_fn=<MeanBackward0>)  loss:  0\n",
            "timestep:  2  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  2  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  2  mask_loss:  tensor(10.3083, grad_fn=<MeanBackward0>)  loss:  tensor(10.3290, grad_fn=<AddBackward0>)\n",
            "timestep:  3  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  3  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  3  mask_loss:  tensor(10.3484, grad_fn=<MeanBackward0>)  loss:  tensor(20.6373, grad_fn=<AddBackward0>)\n",
            "timestep:  4  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  4  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  4  mask_loss:  tensor(10.3247, grad_fn=<MeanBackward0>)  loss:  tensor(30.9857, grad_fn=<AddBackward0>)\n",
            "timestep:  5  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  5  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  5  mask_loss:  tensor(10.3210, grad_fn=<MeanBackward0>)  loss:  tensor(41.3105, grad_fn=<AddBackward0>)\n",
            "timestep:  6  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  6  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  6  mask_loss:  tensor(10.3431, grad_fn=<MeanBackward0>)  loss:  tensor(51.6315, grad_fn=<AddBackward0>)\n",
            "timestep:  7  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  7  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  7  mask_loss:  tensor(10.3376, grad_fn=<MeanBackward0>)  loss:  tensor(61.9746, grad_fn=<AddBackward0>)\n",
            "timestep:  8  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  8  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  8  mask_loss:  tensor(10.3006, grad_fn=<MeanBackward0>)  loss:  tensor(72.3122, grad_fn=<AddBackward0>)\n",
            "timestep:  9  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  9  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  9  mask_loss:  tensor(10.3279, grad_fn=<MeanBackward0>)  loss:  tensor(82.6128, grad_fn=<AddBackward0>)\n",
            "timestep:  10  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  10  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  10  mask_loss:  tensor(10.3189, grad_fn=<MeanBackward0>)  loss:  tensor(92.9406, grad_fn=<AddBackward0>)\n",
            "timestep:  11  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  11  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  11  mask_loss:  tensor(10.2825, grad_fn=<MeanBackward0>)  loss:  tensor(103.2595, grad_fn=<AddBackward0>)\n",
            "timestep:  12  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  12  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  12  mask_loss:  tensor(10.2996, grad_fn=<MeanBackward0>)  loss:  tensor(113.5420, grad_fn=<AddBackward0>)\n",
            "timestep:  13  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  13  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  13  mask_loss:  tensor(10.3019, grad_fn=<MeanBackward0>)  loss:  tensor(123.8416, grad_fn=<AddBackward0>)\n",
            "timestep:  14  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  14  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  14  mask_loss:  tensor(10.2444, grad_fn=<MeanBackward0>)  loss:  tensor(134.1435, grad_fn=<AddBackward0>)\n",
            "timestep:  15  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  15  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  15  mask_loss:  tensor(10.2931, grad_fn=<MeanBackward0>)  loss:  tensor(144.3879, grad_fn=<AddBackward0>)\n",
            "timestep:  16  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  16  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  16  mask_loss:  tensor(10.1878, grad_fn=<MeanBackward0>)  loss:  tensor(154.6810, grad_fn=<AddBackward0>)\n",
            "timestep:  17  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  17  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  17  mask_loss:  tensor(10.2945, grad_fn=<MeanBackward0>)  loss:  tensor(164.8688, grad_fn=<AddBackward0>)\n",
            "timestep:  18  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  18  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  18  mask_loss:  tensor(10.2043, grad_fn=<MeanBackward0>)  loss:  tensor(175.1632, grad_fn=<AddBackward0>)\n",
            "timestep:  19  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  19  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  19  mask_loss:  tensor(10.1987, grad_fn=<MeanBackward0>)  loss:  tensor(185.3675, grad_fn=<AddBackward0>)\n",
            "timestep:  20  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  20  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  20  mask_loss:  tensor(10.3083, grad_fn=<MeanBackward0>)  loss:  tensor(195.5661, grad_fn=<AddBackward0>)\n",
            "timestep:  21  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  21  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  21  mask_loss:  tensor(10.3017, grad_fn=<MeanBackward0>)  loss:  tensor(205.8745, grad_fn=<AddBackward0>)\n",
            "timestep:  22  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  22  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  22  mask_loss:  tensor(10.1069, grad_fn=<MeanBackward0>)  loss:  tensor(216.1762, grad_fn=<AddBackward0>)\n",
            "timestep:  23  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  23  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  23  mask_loss:  tensor(10.3264, grad_fn=<MeanBackward0>)  loss:  tensor(226.2831, grad_fn=<AddBackward0>)\n",
            "timestep:  24  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  24  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  24  mask_loss:  tensor(10.2457, grad_fn=<MeanBackward0>)  loss:  tensor(236.6095, grad_fn=<AddBackward0>)\n",
            "timestep:  25  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  25  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  25  mask_loss:  tensor(9.9159, grad_fn=<MeanBackward0>)  loss:  tensor(246.8552, grad_fn=<AddBackward0>)\n",
            "timestep:  26  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  26  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  26  mask_loss:  tensor(10.2828, grad_fn=<MeanBackward0>)  loss:  tensor(256.7712, grad_fn=<AddBackward0>)\n",
            "timestep:  27  inside train function1:  torch.Size([1, 32, 768])\n",
            "timestep:  27  inside train function2:  torch.Size([1, 32, 768])\n",
            "timestep :  27  mask_loss:  tensor(9.9063, grad_fn=<MeanBackward0>)  loss:  tensor(267.0540, grad_fn=<AddBackward0>)\n",
            "inside trainIters after calling train\n",
            "Epoch:  1  Iteration:  1  avg_loss:  10.296386260447148\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8y7swQ0zpnh"
      },
      "source": [
        "'''\n",
        "beam search node\n",
        "'''\n",
        "class BeamSearchNode(object):\n",
        "    def __init__(self, hiddenstate, previousNode, wordId, logProb, length):\n",
        "        '''\n",
        "        :param hiddenstate:\n",
        "        :param previousNode:\n",
        "        :param wordId:\n",
        "        :param logProb:\n",
        "        :param length:\n",
        "        '''\n",
        "        self.h = hiddenstate\n",
        "        self.prevNode = previousNode\n",
        "        self.wordid = wordId\n",
        "        self.logp = logProb\n",
        "        self.leng = length\n",
        "\n",
        "    def eval(self, alpha=1.0):\n",
        "        reward = 0\n",
        "        return self.logp / float(self.leng - 1 + 1e-6) + alpha * reward\n",
        "\n",
        "    def __lt__(self, other):\n",
        "        return self.leng < other.leng\n",
        "\n",
        "    def __gt__(self, other):\n",
        "        return self.leng > other.leng"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSzMrLSOgJ0M"
      },
      "source": [
        "'''\n",
        "get context token for a beam search node\n",
        "'''\n",
        "def get_context_tokens(beam_search_node):\n",
        "  context_tokens=[]\n",
        "  while(beam_search_node != None):\n",
        "    word_id=beam_search_node.wordid\n",
        "    context_tokens.append(word_id)\n",
        "    beam_search_node=beam_search_node.prevNode\n",
        "  context_tokens.reverse()\n",
        "  return context_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTd2yTI-gZwc"
      },
      "source": [
        "'''\n",
        "beam search decoder\n",
        "'''\n",
        "def beam_decode(decoder, target_tensor, decoder_hiddens, encoder_outputs=None):\n",
        "        '''\n",
        "        :param target_tensor: target indexes tensor of shape [B, T] where B is the batch size and T is the maximum length of the output sentence\n",
        "        :param decoder_hiddens: input tensor of shape [1, B, H] for start of the decoding\n",
        "        :param encoder_outputs: if you are using attention mechanism you can pass encoder outputs, [T, B, H] where T is the maximum length of input sentence\n",
        "        :return: decoded_batch\n",
        "        '''\n",
        "        #target_tensor = target_tensor.permute(1, 0)\n",
        "        batch_size = target_tensor.shape[1]\n",
        "        max_seq_len = target_tensor.shape[0]\n",
        "        #print('1. inside beam decode function batch_size: ',batch_size)\n",
        "        beam_width = 10\n",
        "        topk = 3 # how many sentence do you want to generate\n",
        "        decoded_batch = []\n",
        "\n",
        "        # decoding goes sentence by sentence\n",
        "        for idx in range(batch_size):  # batch_size\n",
        "            if isinstance(decoder_hiddens, tuple):  # LSTM case\n",
        "                decoder_hidden = (\n",
        "                    decoder_hiddens[0][:, idx, :].unsqueeze(0), decoder_hiddens[1][:, idx, :].unsqueeze(0))\n",
        "            else:\n",
        "                decoder_hidden = decoder_hiddens[:, idx, :].unsqueeze(0)  # [1, B, H]=>[1,H]=>[1,1,H] #final hidden vector coming out from encoder for current seq\n",
        "            encoder_output = encoder_outputs[:, idx, :].unsqueeze(1)  # [T,B,H]=>[T,H]=>[T,1,H] #encoder's output coming out from encoder for current seq\n",
        "\n",
        "            # Start with the start of the sentence token\n",
        "            #decoder_input = torch.LongTensor([SOS_token]).cuda()\n",
        "            decoder_input_token=torch.LongTensor([tokenizer.eos_token_id])\n",
        "            decoder_input = get_initial_decoder_ip(1) # here we are processing for one seq of batch so passing batch_size=1 \n",
        "            #decoder_input = torch.ones(1, device=device, dtype=torch.long) * SOS_token\n",
        "            # print('1. inside function beam_decode  shape of decoder_input: ',decoder_input.shape)\n",
        "\n",
        "            # Number of sentence to generate\n",
        "            endnodes = []\n",
        "            number_required = min((topk + 1), topk - len(endnodes))\n",
        "\n",
        "            # starting node -  hidden vector, previous node, word id, logp, length\n",
        "            node = BeamSearchNode(decoder_hidden, None, decoder_input_token, 0, 1)\n",
        "            nodes = PriorityQueue()\n",
        "\n",
        "            # start the queue\n",
        "            nodes.put((-node.eval(), node))\n",
        "            qsize = 1\n",
        "\n",
        "            loop_count=0\n",
        "\n",
        "            # start beam search\n",
        "            while True:\n",
        "                #print('current loop_count is: ',loop_count,' size of priority queue: ',nodes.qsize())\n",
        "                '''\n",
        "                if(timestep==max_seq_len):\n",
        "                  break\n",
        "                '''\n",
        "                # give up when decoding takes too long\n",
        "                if qsize > 2000: break\n",
        "\n",
        "                # fetch the best node\n",
        "                #print('priorityQueue size before retrieving: ',nodes.qsize())\n",
        "                score, n = nodes.get()\n",
        "                #print('priorityQueue size after retrieving: ',nodes.qsize())\n",
        "                decoder_input_token = n.wordid.item()\n",
        "                decoder_hidden = n.h\n",
        "                #print('2. inside function beam_decode  tokenizer.eos_token_id: ',(decoder_input_token,n.prevNode))\n",
        "                if decoder_input_token == tokenizer.eos_token_id and n.prevNode != None:\n",
        "                    #print('decoder_input_token is: ',decoder_input_token,' number_required: ',number_required)\n",
        "                    endnodes.append((score, n))\n",
        "                    # if we reached maximum # of sentences required\n",
        "                    if len(endnodes) >= number_required:\n",
        "                        break\n",
        "                    else:\n",
        "                        continue\n",
        "                context_tokens = get_context_tokens(n)\n",
        "                # print('1. context_tokens: ',context_tokens)\n",
        "                last_token_index=len(context_tokens)-1\n",
        "                with torch.no_grad():\n",
        "                  last_hidden_states = model(torch.tensor([context_tokens]))\n",
        "                bert_vector__for_context_tokens=last_hidden_states[0]#(batch_size*max_op_len*768)\n",
        "                # print('2. shape of ip to bert encoding: ',torch.tensor([context_tokens]).shape)\n",
        "                # print('3. bert_vector__for_context_tokens shape: ',bert_vector__for_context_tokens.shape)\n",
        "                decoder_input=torch.unsqueeze(bert_vector__for_context_tokens.permute(1,0,2)[last_token_index],0)\n",
        "                loop_count=loop_count+1\n",
        "                # print('4. before decoder forward pass decoder_input shape',decoder_input.shape)\n",
        "                #decoder_input = torch.unsqueeze(decoder_input, 0)\n",
        "                # decode for one step using decoder\n",
        "                decoder_output, decoder_hidden =  decoder(decoder_input, decoder_hidden, encoder_output)\n",
        "                # PUT HERE REAL BEAM SEARCH OF TOP\n",
        "                log_prob, indexes = torch.topk(decoder_output, beam_width)#top 'beam_width(10)' log probability arranged in descending order and their indexes\n",
        "                #print('after decoder forward pass (log_prob, indexes)',(log_prob, indexes))\n",
        "                nextnodes = []\n",
        "\n",
        "                for new_k in range(beam_width):\n",
        "                    decoded_t = indexes[0][new_k].view(-1)\n",
        "                    log_p = log_prob[0][new_k].item()\n",
        "\n",
        "                    node = BeamSearchNode(decoder_hidden, n, decoded_t, n.logp + log_p, n.leng + 1)#hidden vector, previous node, word id, logp, length\n",
        "                    score = -node.eval()\n",
        "                    nextnodes.append((score, node))\n",
        "\n",
        "                # put them into queue\n",
        "                for i in range(len(nextnodes)):\n",
        "                    score, nn = nextnodes[i]\n",
        "                    nodes.put((score, nn))\n",
        "                    # increase qsize\n",
        "                qsize += len(nextnodes) - 1\n",
        "\n",
        "            # choose nbest paths, back trace them\n",
        "            if len(endnodes) == 0:\n",
        "                endnodes = [nodes.get() for _ in range(topk)]\n",
        "\n",
        "            utterances = []\n",
        "            for score, n in sorted(endnodes, key=operator.itemgetter(0)):\n",
        "                utterance = []\n",
        "                utterance.append(n.wordid)\n",
        "                # back trace\n",
        "                while n.prevNode != None:\n",
        "                    n = n.prevNode\n",
        "                    utterance.append(n.wordid)\n",
        "\n",
        "                utterance = utterance[::-1]\n",
        "                utterances.append(utterance)\n",
        "\n",
        "            decoded_batch.append(utterances)\n",
        "\n",
        "        return decoded_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVerpbN5ghzU"
      },
      "source": [
        "'''\n",
        "greedy search decoder\n",
        "'''\n",
        "def greedy_decode(decoder,trg, decoder_hidden, encoder_outputs, ):\n",
        "        '''\n",
        "        :param target_tensor: target indexes tensor of shape [B, T] where B is the batch size and T is the maximum length of the output sentence\n",
        "        :param decoder_hidden: input tensor of shape [1, B, H] for start of the decoding\n",
        "        :param encoder_outputs: if you are using attention mechanism you can pass encoder outputs, [T, B, H] where T is the maximum length of input sentence\n",
        "        :return: decoded_batch\n",
        "        '''\n",
        "        seq_len, batch_size = trg.size()\n",
        "\n",
        "        decoded_batch = torch.zeros((batch_size, seq_len)).int()\n",
        "        #print((decoded_batch.dtype))\n",
        "        # decoder_input = torch.LongTensor([[EN.vocab.stoi['<sos>']] for _ in range(batch_size)]).cuda()\n",
        "        #decoder_input = Variable(trg.data[0, :])  # sos\n",
        "        decoder_input = get_initial_decoder_ip(batch_size)\n",
        "        #print('1. inside greedy_decode decoder_input shape : ',decoder_input.shape)\n",
        "        for timestep in range(seq_len):\n",
        "            decoder_output, decoder_hidden= decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            #print('2. inside greedy_decode decoder_output shape : ',decoder_output.shape)# [1, 30521]\n",
        "            #print('3. inside greedy_decode decoder_hidden shape : ',decoder_hidden.shape)\n",
        "            topv, topi = decoder_output.data.topk(1)  \n",
        "            #print('topv, topi: ',(topv, topi))\n",
        "            topi = topi.view(-1)\n",
        "            decoded_batch[:, timestep] = topi   \n",
        "            attention_mask_decoded_batch = torch.where(decoded_batch != 0, 1, 0)        \n",
        "            decoder_input = topi.detach().view(-1)\n",
        "            #print('4 inside greedy_decode decoder_input 2 : ',decoder_input)\n",
        "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
        "            #print('5 inside greedy_decode decoder_input 3 : ',decoder_input)\n",
        "            with torch.no_grad():\n",
        "              last_hidden_states = model(decoded_batch, attention_mask=attention_mask_decoded_batch)\n",
        "            decoded_batch_vector=last_hidden_states[0]#(batch_size*max_op_len*768)\n",
        "            decoder_input=torch.unsqueeze(decoded_batch_vector.permute(1,0,2)[timestep],0)\n",
        "            #print('6 inside greedy_decode decoder_input 5 : ',decoder_input.shape)\n",
        "        return decoded_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2EuGMFegwln"
      },
      "source": [
        "'''\n",
        "decode based on provided search(greedy/beam) method\n",
        "'''\n",
        "def decode(encoder, decoder,input_seq, input_length, trg, method='beam-search'):\n",
        "    # Forward input through encoder model\n",
        "    encoder_outputs, encoder_hidden = encoder(input_seq, input_length)\n",
        "    #encoder_output, encoder_hidden = encoder(src)  # [27, 32]=> =>[27, 32, 512],[4, 32, 512]\n",
        "    encoder_hidden = encoder_hidden[:decoder.num_layers]  # [4, 32, 512][1, 32, 512]\n",
        "    #print('inside decode function encoder_outputs shape: ',encoder_outputs.shape)\n",
        "    #print('inside decode function encoder_hidden shape: ',encoder_hidden.shape)\n",
        "    #print('inside decode function trg shape: ',trg.shape)\n",
        "    if method == 'beam-search':\n",
        "        return beam_decode(decoder,trg, encoder_hidden, encoder_outputs)\n",
        "    else:\n",
        "        return greedy_decode(decoder,trg, encoder_hidden, encoder_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Eivev4cgzsx"
      },
      "source": [
        "'''\n",
        "function for generating op sequence for a given input seq\n",
        "'''\n",
        "MAX_LENGTH=15\n",
        "def evaluate(encoder, decoder, summary_batch_vector, summary_length_batch, max_length=MAX_LENGTH,search_method='greedy-search'):\n",
        "  #input_variable,lengths,target_variable,target_op_token_idxs,mask =batch_data.batch_ip_vector,batch_data.batch_ip_length,batch_data.batch_op_vector,batch_data.batch_op_token_idxs,batch_data.batch_mask\n",
        "  input_variable=summary_batch_vector.permute(1,0,2)\n",
        "  lengths = torch.tensor(summary_length_batch)\n",
        "  #target_variable=target_variable.permute(1,0,2)\n",
        "  #target_op_token_idxs=target_op_token_idxs.permute(1,0)\n",
        "  #mask=mask.permute(1,0)\n",
        "  trg=torch.zeros([max_length,1], dtype=torch.long).to(device)\n",
        "  if (search_method=='greedy-search'):\n",
        "    decoded_batch=decode(encoder, decoder,input_variable, lengths, trg, method='greedy-search')\n",
        "    #print('decoded_batch: ',decoded_batch)\n",
        "    decoded_words = [tokenizer.decode(token_idx) for token_idx in decoded_batch[0]]\n",
        "  else:\n",
        "    decoded_batches=decode(encoder, decoder,input_variable, lengths, trg, method='beam-search')\n",
        "    decoded_batches=decoded_batches[0]\n",
        "    #print('decoded_batch: ',decoded_batches)\n",
        "    decoded_words=[]\n",
        "    for decoded_batch in decoded_batches:\n",
        "      decoded_words.append([tokenizer.decode(token_idx) for token_idx in decoded_batch])\n",
        "    print('decoded_words: ',decoded_words)\n",
        "  return decoded_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTe3GOcgg4p2",
        "outputId": "750f8473-2e9a-41bb-f8ef-1a29413f61c8"
      },
      "source": [
        "  '''\n",
        "  generate sequence for a given input news summary using 'beam-search' \n",
        "  '''\n",
        "  summary_headline=summary_headline_pairs[12]\n",
        "  content=summary_headline[0]\n",
        "  ref_headline=summary_headline[1]\n",
        "  print(\"article content: \")\n",
        "  print(content)\n",
        "  print(\"reference headline: \",ref_headline)\n",
        "\n",
        "  summary_test=[content]\n",
        "  summary_test_tokens=[tokenizer.encode(summary, add_special_tokens=True) for summary in summary_test]\n",
        "  summary_test_tokens=sorted(summary_test_tokens, key=lambda x: len(x), reverse=True)#sort summaries based on no of tokens in them\n",
        "  #print('****************** no of tokens in test summary: ',len(summary_test_tokens[0]))\n",
        "  \n",
        "  #adding PAD token to get uniform size\n",
        "  PAD_TOKEN=0 #BERT tokenizer.decode(0) will give you 'PAD' as a token\n",
        "  summary_max_len = 0\n",
        "  for summary_token in summary_test_tokens:\n",
        "    if len(summary_token) > summary_max_len:\n",
        "        summary_max_len = len(summary_token)\n",
        "\n",
        "  #print('maximum no of tokens for a summary sequence in given batch: ',summary_max_len)\n",
        "  summary_max_len=125 # TODO remove this line and check ideally it should be computed 'summary_max_len'\n",
        "\n",
        "  padded_summaries = np.array([summary_token + [PAD_TOKEN]*(summary_max_len-len(summary_token)) for summary_token in summary_test_tokens])\n",
        "  #print('after padding summary data shape: ',padded_summaries.shape)\n",
        "  summary_batch=padded_summaries\n",
        "  summary_length_batch=[np.count_nonzero(summary!=0) for summary in summary_batch]\n",
        "  attention_mask_summary = np.where(summary_batch != 0, 1, 0)\n",
        "  attention_mask_summary_t = torch.tensor(attention_mask_summary)\n",
        "  summary_batch_t = torch.tensor(summary_batch) \n",
        "  #print('summary_length_batch',summary_length_batch)\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    last_hidden_states = model(summary_batch_t, attention_mask=attention_mask_summary_t)\n",
        "  summary_batch_vector=last_hidden_states[0]\n",
        "\n",
        "  #print('summary_batch_vector shape: ',summary_batch_vector.shape)\n",
        "\n",
        "  ##\n",
        "  op_headlines=evaluate(encoder, decoder, summary_batch_vector, summary_length_batch, max_length=MAX_LENGTH,search_method='beam-search')\n",
        "  print('generated headline from beam search decoding: ')\n",
        "  for op_headline in op_headlines:\n",
        "    print('generated headlie: ', \" \".join(op_headline))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "article content: \n",
            "Boeing CEO David Calhoun received a total compensation of over $21 million in 2020 after he gave up $3.6 million in the form of salary and bonuses amid the COVID-19 pandemic. He took $269,231 in salary for the period before he disavowed his salary in March but received around $20 million in the form of stock benefits.\n",
            "reference headline:  Despite Waiving Salary, Boeing CEO Made $21M Last Year\n",
            "inside encoder forward function || input_seq shape:  torch.Size([125, 1, 768])\n",
            "inside encoder forward function || input_lengths shape:  torch.Size([1])\n",
            "inside encoder forward function || outputs shape:  torch.Size([125, 1, 256])\n",
            "inside encoder forward function || hidden shape:  torch.Size([1, 1, 256])\n",
            "decoder_input shape:  torch.Size([1, 1, 768])\n",
            "decoder_input shape:  torch.Size([1, 1, 768])\n",
            "decoded_words:  [['[SEP]', 'symposium', 'ர', 'ர', 'ர', 'ர', 'ர', '##mas', '[SEP]'], ['[SEP]', 'symposium', 'ர', 'ர', 'ர', 'ர', 'ர', 'symposium', '[SEP]'], ['[SEP]', 'symposium', 'ர', 'ர', 'ர', 'ர', 'ர', 'revolves', '[SEP]']]\n",
            "generated headline from beam search decoding: \n",
            "generated headlie:  [SEP] symposium ர ர ர ர ர ##mas [SEP]\n",
            "generated headlie:  [SEP] symposium ர ர ர ர ர symposium [SEP]\n",
            "generated headlie:  [SEP] symposium ர ர ர ர ர revolves [SEP]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hT9LisSBhVRH",
        "outputId": "883ae9d8-d2ee-46db-e997-41cc4a2013c4"
      },
      "source": [
        "  '''\n",
        "  generate sequence for a given input news summary using 'greedy-search' \n",
        "  '''\n",
        "  summary_headline=summary_headline_pairs[12]\n",
        "  content=summary_headline[0]\n",
        "  ref_headline=summary_headline[1]\n",
        "  print(\"article content: \")\n",
        "  print(content)\n",
        "  print(\"reference headline: \",ref_headline)\n",
        "\n",
        "  summary_test=[content]\n",
        "  summary_test_tokens=[tokenizer.encode(summary, add_special_tokens=True) for summary in summary_test]\n",
        "  summary_test_tokens=sorted(summary_test_tokens, key=lambda x: len(x), reverse=True)#sort summaries based on no of tokens in them\n",
        "  #print('****************** no of tokens in test summary: ',len(summary_test_tokens[0]))\n",
        "  \n",
        "  #adding PAD token to get uniform size\n",
        "  PAD_TOKEN=0 #BERT tokenizer.decode(0) will give you 'PAD' as a token\n",
        "  summary_max_len = 0\n",
        "  for summary_token in summary_test_tokens:\n",
        "    if len(summary_token) > summary_max_len:\n",
        "        summary_max_len = len(summary_token)\n",
        "\n",
        "  #print('maximum no of tokens for a summary sequence in given batch: ',summary_max_len)\n",
        "  summary_max_len=125 # TODO remove this line and check ideally it should be computed 'summary_max_len'\n",
        "\n",
        "  padded_summaries = np.array([summary_token + [PAD_TOKEN]*(summary_max_len-len(summary_token)) for summary_token in summary_test_tokens])\n",
        "  #print('after padding summary data shape: ',padded_summaries.shape)\n",
        "  summary_batch=padded_summaries\n",
        "  summary_length_batch=[np.count_nonzero(summary!=0) for summary in summary_batch]\n",
        "  attention_mask_summary = np.where(summary_batch != 0, 1, 0)\n",
        "  attention_mask_summary_t = torch.tensor(attention_mask_summary)\n",
        "  summary_batch_t = torch.tensor(summary_batch) \n",
        "  #print('summary_length_batch',summary_length_batch)\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    last_hidden_states = model(summary_batch_t, attention_mask=attention_mask_summary_t)\n",
        "  summary_batch_vector=last_hidden_states[0]\n",
        "\n",
        "  #print('summary_batch_vector shape: ',summary_batch_vector.shape)\n",
        "\n",
        "  ##\n",
        "  op_headline=evaluate(encoder, decoder, summary_batch_vector, summary_length_batch, max_length=MAX_LENGTH,search_method='greedy-search')\n",
        "  print(op_headline)\n",
        "  print('generated headline from greedy search decoding: ', \" \".join(op_headline))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "article content: \n",
            "Boeing CEO David Calhoun received a total compensation of over $21 million in 2020 after he gave up $3.6 million in the form of salary and bonuses amid the COVID-19 pandemic. He took $269,231 in salary for the period before he disavowed his salary in March but received around $20 million in the form of stock benefits.\n",
            "reference headline:  Despite Waiving Salary, Boeing CEO Made $21M Last Year\n",
            "inside encoder forward function || input_seq shape:  torch.Size([125, 1, 768])\n",
            "inside encoder forward function || input_lengths shape:  torch.Size([1])\n",
            "inside encoder forward function || outputs shape:  torch.Size([125, 1, 256])\n",
            "inside encoder forward function || hidden shape:  torch.Size([1, 1, 256])\n",
            "decoder_input shape:  torch.Size([1, 1, 768])\n",
            "decoder_input shape:  torch.Size([1, 1, 768])\n",
            "['[ S E P ]', 'ர', 'ர', 'i n t u i t i v e', '# # m a s', '[ S E P ]', 'r e v o l v e s', 'r e v o l v e s', '[ S E P ]', 'p h o s p h o r u s', '[ S E P ]', 'p h o s p h o r u s', '[ S E P ]', '[ S E P ]', 'c h o r e s']\n",
            "generated headline from greedy search decoding:  [ S E P ] ர ர i n t u i t i v e # # m a s [ S E P ] r e v o l v e s r e v o l v e s [ S E P ] p h o s p h o r u s [ S E P ] p h o s p h o r u s [ S E P ] [ S E P ] c h o r e s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrMV0Xqyj28r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}