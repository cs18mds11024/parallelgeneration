{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "one_to_many_setup.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SCTeXzBcJuG",
        "outputId": "75793bc0-fd20-4155-c49a-c90a919a63c1"
      },
      "source": [
        "'''\n",
        "mount google drive\n",
        "'''\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULR4MQm-d3E3",
        "outputId": "435c82d3-d3e7-48a6-faaf-d280cadb46d9"
      },
      "source": [
        "'''\n",
        "install required libraries\n",
        "'''\n",
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl (2.5MB)\n",
            "\r\u001b[K     |▏                               | 10kB 16.3MB/s eta 0:00:01\r\u001b[K     |▎                               | 20kB 20.1MB/s eta 0:00:01\r\u001b[K     |▍                               | 30kB 23.4MB/s eta 0:00:01\r\u001b[K     |▌                               | 40kB 25.8MB/s eta 0:00:01\r\u001b[K     |▋                               | 51kB 26.6MB/s eta 0:00:01\r\u001b[K     |▉                               | 61kB 27.7MB/s eta 0:00:01\r\u001b[K     |█                               | 71kB 28.5MB/s eta 0:00:01\r\u001b[K     |█                               | 81kB 28.6MB/s eta 0:00:01\r\u001b[K     |█▏                              | 92kB 26.6MB/s eta 0:00:01\r\u001b[K     |█▎                              | 102kB 27.5MB/s eta 0:00:01\r\u001b[K     |█▍                              | 112kB 27.5MB/s eta 0:00:01\r\u001b[K     |█▋                              | 122kB 27.5MB/s eta 0:00:01\r\u001b[K     |█▊                              | 133kB 27.5MB/s eta 0:00:01\r\u001b[K     |█▉                              | 143kB 27.5MB/s eta 0:00:01\r\u001b[K     |██                              | 153kB 27.5MB/s eta 0:00:01\r\u001b[K     |██                              | 163kB 27.5MB/s eta 0:00:01\r\u001b[K     |██▎                             | 174kB 27.5MB/s eta 0:00:01\r\u001b[K     |██▍                             | 184kB 27.5MB/s eta 0:00:01\r\u001b[K     |██▌                             | 194kB 27.5MB/s eta 0:00:01\r\u001b[K     |██▋                             | 204kB 27.5MB/s eta 0:00:01\r\u001b[K     |██▊                             | 215kB 27.5MB/s eta 0:00:01\r\u001b[K     |██▉                             | 225kB 27.5MB/s eta 0:00:01\r\u001b[K     |███                             | 235kB 27.5MB/s eta 0:00:01\r\u001b[K     |███▏                            | 245kB 27.5MB/s eta 0:00:01\r\u001b[K     |███▎                            | 256kB 27.5MB/s eta 0:00:01\r\u001b[K     |███▍                            | 266kB 27.5MB/s eta 0:00:01\r\u001b[K     |███▌                            | 276kB 27.5MB/s eta 0:00:01\r\u001b[K     |███▊                            | 286kB 27.5MB/s eta 0:00:01\r\u001b[K     |███▉                            | 296kB 27.5MB/s eta 0:00:01\r\u001b[K     |████                            | 307kB 27.5MB/s eta 0:00:01\r\u001b[K     |████                            | 317kB 27.5MB/s eta 0:00:01\r\u001b[K     |████▏                           | 327kB 27.5MB/s eta 0:00:01\r\u001b[K     |████▎                           | 337kB 27.5MB/s eta 0:00:01\r\u001b[K     |████▌                           | 348kB 27.5MB/s eta 0:00:01\r\u001b[K     |████▋                           | 358kB 27.5MB/s eta 0:00:01\r\u001b[K     |████▊                           | 368kB 27.5MB/s eta 0:00:01\r\u001b[K     |████▉                           | 378kB 27.5MB/s eta 0:00:01\r\u001b[K     |█████                           | 389kB 27.5MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 399kB 27.5MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 409kB 27.5MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 419kB 27.5MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 430kB 27.5MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 440kB 27.5MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 450kB 27.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 460kB 27.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 471kB 27.5MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 481kB 27.5MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 491kB 27.5MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 501kB 27.5MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 512kB 27.5MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 522kB 27.5MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 532kB 27.5MB/s eta 0:00:01\r\u001b[K     |███████                         | 542kB 27.5MB/s eta 0:00:01\r\u001b[K     |███████                         | 552kB 27.5MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 563kB 27.5MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 573kB 27.5MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 583kB 27.5MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 593kB 27.5MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 604kB 27.5MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 614kB 27.5MB/s eta 0:00:01\r\u001b[K     |████████                        | 624kB 27.5MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 634kB 27.5MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 645kB 27.5MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 655kB 27.5MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 665kB 27.5MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 675kB 27.5MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 686kB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████                       | 696kB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████                       | 706kB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 716kB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 727kB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 737kB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 747kB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 757kB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 768kB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████                      | 778kB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████                      | 788kB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 798kB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 808kB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 819kB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 829kB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 839kB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 849kB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 860kB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 870kB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 880kB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 890kB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 901kB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 911kB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 921kB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████                    | 931kB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████                    | 942kB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 952kB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 962kB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 972kB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 983kB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 993kB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 1.0MB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 1.0MB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 1.0MB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 1.0MB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 1.0MB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 1.1MB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 1.1MB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 1.1MB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 1.1MB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 1.1MB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 1.1MB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 1.1MB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 1.1MB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 1.1MB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 1.1MB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 1.2MB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.2MB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.2MB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 1.2MB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 1.2MB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 1.2MB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 1.2MB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 1.2MB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 1.2MB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.2MB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 1.3MB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 1.3MB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 1.3MB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 1.3MB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 1.3MB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 1.3MB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.3MB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.3MB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 1.3MB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 1.4MB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 1.4MB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 1.4MB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 1.4MB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 1.4MB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.4MB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.4MB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 1.4MB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 1.4MB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.4MB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 1.5MB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.5MB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.5MB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.5MB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 1.5MB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.5MB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.5MB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 1.5MB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.5MB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.5MB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.6MB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.6MB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.6MB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 1.6MB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.6MB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.6MB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.6MB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.6MB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.6MB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.6MB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.7MB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.7MB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.7MB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.7MB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.7MB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.7MB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.7MB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.7MB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.7MB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.8MB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.8MB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.8MB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.8MB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.8MB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.8MB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.8MB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.8MB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.8MB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.8MB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.9MB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.9MB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.9MB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.9MB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.9MB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.9MB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.9MB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.9MB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.9MB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.9MB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 2.0MB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 2.0MB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 2.0MB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 2.0MB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 2.0MB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 2.0MB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 2.0MB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 2.0MB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 2.0MB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 2.0MB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 2.1MB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 2.1MB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 2.1MB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 2.1MB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 2.1MB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 2.1MB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 2.1MB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 2.1MB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 2.1MB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 2.2MB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 2.2MB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 2.2MB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 2.2MB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 2.2MB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 2.2MB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 2.2MB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 2.2MB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 2.2MB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 2.2MB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 2.3MB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 2.3MB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 2.3MB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 2.3MB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 2.3MB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 2.3MB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 2.3MB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 2.3MB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 2.3MB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 2.3MB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 2.4MB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 2.4MB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 2.4MB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 2.4MB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 2.4MB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 2.4MB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.4MB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.4MB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 2.4MB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 2.4MB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 2.5MB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 2.5MB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 2.5MB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 2.5MB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.5MB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.5MB 27.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 33.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 40.4MB/s \n",
            "\u001b[?25hCollecting huggingface-hub==0.0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBBuSRg6d5sH",
        "outputId": "94747b67-9510-4060-d5dc-26aaac8f1867"
      },
      "source": [
        "'''\n",
        "import required packages\n",
        "'''\n",
        "import unicodedata\n",
        "import re\n",
        "import os\n",
        "import random\n",
        "import string\n",
        "import itertools\n",
        "import pickle\n",
        "import glob\n",
        "\n",
        "from queue import PriorityQueue\n",
        "import operator\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import transformers\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords  \n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blgpDRNwd8m5"
      },
      "source": [
        "'''\n",
        "configuration for deterministic results with multiple run\n",
        "'''\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed) \n",
        "\n",
        "np.random.seed(seed)  \n",
        "random.seed(seed) \n",
        "\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GfsPQhId_w3"
      },
      "source": [
        "'''\n",
        "pandas configuration for showing complete content of record\n",
        "'''\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 2000)\n",
        "pd.set_option('display.float_format', '{:20,.2f}'.format)\n",
        "pd.set_option('display.max_colwidth', None)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 702
        },
        "id": "5Pe2ewPHIoHx",
        "outputId": "160c17a1-7ccf-43bc-e708-64e491afe6a9"
      },
      "source": [
        "'''\n",
        "load news data and show some sample \n",
        "'''\n",
        "data_path='/content/gdrive/My Drive/Capstone_Project/Data/News_Data/news_article_with_sim_score.df'\n",
        "article_df=pd.read_pickle(data_path)\n",
        "no_of_headlines=[len(similar_headlines) for similar_headlines in article_df['similar_headline'].tolist()]\n",
        "print('max no of similar headlines: ',max(no_of_headlines))\n",
        "print('min no of similar headlines: ',min(no_of_headlines))\n",
        "print('article_df shape:',article_df.shape)\n",
        "article_df.sample(2)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max no of similar headlines:  13\n",
            "min no of similar headlines:  0\n",
            "article_df shape: (3000, 9)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article_url</th>\n",
              "      <th>headline</th>\n",
              "      <th>content</th>\n",
              "      <th>author</th>\n",
              "      <th>published_date</th>\n",
              "      <th>read_more_source</th>\n",
              "      <th>similar_headline</th>\n",
              "      <th>similar_headline_url</th>\n",
              "      <th>similarity_scores</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1801</th>\n",
              "      <td>https://inshorts.com/en/news/foot-of-missing-businesswoman-who-stole-₹74cr-from-clients-found-on-aus-beach-1614333830092</td>\n",
              "      <td>Foot of missing businesswoman who stole ₹74cr from clients found on Aus beach</td>\n",
              "      <td>Australian police has said that campers have found the decomposed foot of missing businesswoman Melissa Caddick on a beach. Caddick, who allegedly stole A$13 million (over ₹74 crore) from her clients, disappeared on November 12 last year after federal police raided her home in Sydney. \"She may have taken her own life,\" police added.</td>\n",
              "      <td>None</td>\n",
              "      <td>2021-02-26T10:03:50.000Z</td>\n",
              "      <td>Daily Mail</td>\n",
              "      <td>[Melissa Caddick: Missing fraud suspect's foot found on Australian beach, Melissa Caddick: remains of missing businesswoman found months after disappearance, Melissa Caddick dead, police confirm, after campers find her foot on NSW South Coast, Remains of missing businesswoman and 'conwoman' Melissa Caddick have been found, NSW Health In Australia Orders Radiology Solution From Sectra For Enterprise Access To Images, Sexy Croc &amp;dash Entry #1372 &amp;dash Data Clustering Contest]</td>\n",
              "      <td>[https://www.bbc.com/news/world-australia-56205519, https://www.theguardian.com/australia-news/2021/feb/26/melissa-caddick-missing-financial-adviser-found-dead-months-after-disappearance, https://www.abc.net.au/news/2021-02-26/melissa-caddick-found-dead/13195242, https://www.dailymail.co.uk/news/article-9301259/Remains-missing-businesswoman-conwoman-Melissa-Caddick-found.html, https://www.medicalbuyer.co.in/nsw-health-in-australia-orders-radiology-solution-from-sectra-for-enterprise-access-to-images/, https://entry1372-dcround2.usercontent.dev/20200529/categories/en/economy.html]</td>\n",
              "      <td>[0.58, 0.51, 0.34, 0.49, 0.19, 0.16]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1190</th>\n",
              "      <td>https://inshorts.com/en/news/trumps-gab-account-compromised-as-hackers-target-platform-1614608033197</td>\n",
              "      <td>Trump's Gab account compromised as hackers target platform</td>\n",
              "      <td>Former US President Donald Trump's Gab account was compromised along with the social network Gab CEO Andrew Torba's account. Torba revealed that the platform is being attacked by hackers who had earlier targeted law enforcement officers. According to Wired, around 70 gigabytes of Gab data representing over 40 million posts has been stolen and includes passwords, group passwords and messages.</td>\n",
              "      <td>None</td>\n",
              "      <td>2021-03-01T14:13:53.000Z</td>\n",
              "      <td>Business Insider India</td>\n",
              "      <td>[Far-right social media Gab hacked, Trump's account targeted, Gab confirms it was hacked, Trump and Gab CEO accounts compromised during large-scale hack of alternative social media platform, Gab Hack Reveals Passwords And Private Messages, Hacktivists Attack Controversial Christian Conservative Social Media Site Gab, Leak 70 Gigabytes of Hacked Data Including Private Messages and Passwords, Gab: hack gives unprecedented look into platform used by far right, Gab Founder Andrew Torba Says Platform Was Hacked By Far-Left Activists : US : Christianity Daily, US Right-Wing Platform Gab Acknowledges it Was Hacked, Passwords, Private Posts Exposed in Hack of Gab Social Network]</td>\n",
              "      <td>[https://www.jpost.com/international/far-right-social-media-gab-hacked-trumps-account-targeted-660790, https://www.securitymagazine.com/articles/94733-gab-confirms-it-was-hacked, https://www.coloradopolitics.com/news/trump-and-gab-ceo-accounts-compromised-during-large-scale-hack-of-alternative-social-media-platform/article_379f06da-eb18-5226-b920-0833a591345f.html, https://www.forbes.com/sites/emmawoollacott/2021/03/02/gab-hack-reveals-passwords-and-private-posts/, https://www.cpomagazine.com/cyber-security/hacktivists-attack-controversial-christian-conservative-social-media-site-gab-leak-70-gigabytes-of-hacked-data-including-private-messages-and-passwords/, https://www.theguardian.com/world/2021/mar/11/gab-hack-neo-nazis-qanon-conspiracy-theories, http://www.christianitydaily.com/articles/11022/20210303/gab-founder-andrew-torba-says-platform-was-hacked-by-far-left-activists.htm, https://www.securityweek.com/us-right-wing-platform-gab-acknowledges-it-was-hacked, https://threatpost.com/hacktivists-gab-posts-passwords/164360/]</td>\n",
              "      <td>[0.76, 0.66, 0.86, 0.54, 0.47, 0.59, 0.43, 0.44, 0.54]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                   article_url                                                                       headline                                                                                                                                                                                                                                                                                                                                                                                                     content author            published_date        read_more_source                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         similar_headline  \\\n",
              "1801  https://inshorts.com/en/news/foot-of-missing-businesswoman-who-stole-₹74cr-from-clients-found-on-aus-beach-1614333830092  Foot of missing businesswoman who stole ₹74cr from clients found on Aus beach                                                              Australian police has said that campers have found the decomposed foot of missing businesswoman Melissa Caddick on a beach. Caddick, who allegedly stole A$13 million (over ₹74 crore) from her clients, disappeared on November 12 last year after federal police raided her home in Sydney. \"She may have taken her own life,\" police added.   None  2021-02-26T10:03:50.000Z              Daily Mail                                                                                                                                                                                                           [Melissa Caddick: Missing fraud suspect's foot found on Australian beach, Melissa Caddick: remains of missing businesswoman found months after disappearance, Melissa Caddick dead, police confirm, after campers find her foot on NSW South Coast, Remains of missing businesswoman and 'conwoman' Melissa Caddick have been found, NSW Health In Australia Orders Radiology Solution From Sectra For Enterprise Access To Images, Sexy Croc &dash Entry #1372 &dash Data Clustering Contest]   \n",
              "1190                      https://inshorts.com/en/news/trumps-gab-account-compromised-as-hackers-target-platform-1614608033197                     Trump's Gab account compromised as hackers target platform  Former US President Donald Trump's Gab account was compromised along with the social network Gab CEO Andrew Torba's account. Torba revealed that the platform is being attacked by hackers who had earlier targeted law enforcement officers. According to Wired, around 70 gigabytes of Gab data representing over 40 million posts has been stolen and includes passwords, group passwords and messages.   None  2021-03-01T14:13:53.000Z  Business Insider India  [Far-right social media Gab hacked, Trump's account targeted, Gab confirms it was hacked, Trump and Gab CEO accounts compromised during large-scale hack of alternative social media platform, Gab Hack Reveals Passwords And Private Messages, Hacktivists Attack Controversial Christian Conservative Social Media Site Gab, Leak 70 Gigabytes of Hacked Data Including Private Messages and Passwords, Gab: hack gives unprecedented look into platform used by far right, Gab Founder Andrew Torba Says Platform Was Hacked By Far-Left Activists : US : Christianity Daily, US Right-Wing Platform Gab Acknowledges it Was Hacked, Passwords, Private Posts Exposed in Hack of Gab Social Network]   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  similar_headline_url                                       similarity_scores  \n",
              "1801                                                                                                                                                                                                                                                                                                                                                                                                                                                                        [https://www.bbc.com/news/world-australia-56205519, https://www.theguardian.com/australia-news/2021/feb/26/melissa-caddick-missing-financial-adviser-found-dead-months-after-disappearance, https://www.abc.net.au/news/2021-02-26/melissa-caddick-found-dead/13195242, https://www.dailymail.co.uk/news/article-9301259/Remains-missing-businesswoman-conwoman-Melissa-Caddick-found.html, https://www.medicalbuyer.co.in/nsw-health-in-australia-orders-radiology-solution-from-sectra-for-enterprise-access-to-images/, https://entry1372-dcround2.usercontent.dev/20200529/categories/en/economy.html]                    [0.58, 0.51, 0.34, 0.49, 0.19, 0.16]  \n",
              "1190  [https://www.jpost.com/international/far-right-social-media-gab-hacked-trumps-account-targeted-660790, https://www.securitymagazine.com/articles/94733-gab-confirms-it-was-hacked, https://www.coloradopolitics.com/news/trump-and-gab-ceo-accounts-compromised-during-large-scale-hack-of-alternative-social-media-platform/article_379f06da-eb18-5226-b920-0833a591345f.html, https://www.forbes.com/sites/emmawoollacott/2021/03/02/gab-hack-reveals-passwords-and-private-posts/, https://www.cpomagazine.com/cyber-security/hacktivists-attack-controversial-christian-conservative-social-media-site-gab-leak-70-gigabytes-of-hacked-data-including-private-messages-and-passwords/, https://www.theguardian.com/world/2021/mar/11/gab-hack-neo-nazis-qanon-conspiracy-theories, http://www.christianitydaily.com/articles/11022/20210303/gab-founder-andrew-torba-says-platform-was-hacked-by-far-left-activists.htm, https://www.securityweek.com/us-right-wing-platform-gab-acknowledges-it-was-hacked, https://threatpost.com/hacktivists-gab-posts-passwords/164360/]  [0.76, 0.66, 0.86, 0.54, 0.47, 0.59, 0.43, 0.44, 0.54]  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHvSrrkxNeQA"
      },
      "source": [
        "contents=[]\n",
        "target_headlines_1=[]\n",
        "target_headlines_2=[]\n",
        "target_headlines_3=[]\n",
        "similarity_scores_threshold=0.50\n",
        "for index, row in article_df.iterrows():\n",
        "  similarity_scores=row['similarity_scores']\n",
        "  #print(similarity_scores)\n",
        "  #print(similar_headlines)\n",
        "  sorted_index=list(np.argsort(similarity_scores)) # in ascending order\n",
        "  sorted_index.reverse() # in descending order\n",
        "  if len(sorted_index)>=2:\n",
        "    second_highest_sim_score=similarity_scores[sorted_index[1]]\n",
        "    #print(second_highest_sim_score)\n",
        "    if (second_highest_sim_score >= similarity_scores_threshold):\n",
        "      target_headlines_1.append(row['headline'])\n",
        "      similar_headlines=row['similar_headline']\n",
        "      target_headlines_2.append(similar_headlines[sorted_index[0]]) # first best similar\n",
        "      target_headlines_3.append(similar_headlines[sorted_index[1]]) # second best similar\n",
        "      contents.append(row['content'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-yzOdo4sGYh",
        "outputId": "2c4b6b13-38a8-4c68-a5b0-2a7b180a01f9"
      },
      "source": [
        "#sample record\n",
        "print('news-summary: ',contents[0])\n",
        "print('headlines1: ',target_headlines_1[0])\n",
        "print('headlines2: ',target_headlines_2[0])\n",
        "print('headlines3: ',target_headlines_3[0])\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "news-summary:  Taking to Instagram on Saturday, Arjun Kapoor posted a picture of himself with Janhvi Kapoor to wish the actress on her 24th birthday. In the picture, Arjun can be seen walking ahead while holding his sister's hand. \"Happy birthday Janhvi...I can't promise much except like this picture you shall always have my support & hand wherever you go,\" Arjun wrote.\n",
            "headlines1:  You shall always have my support: Arjun Kapoor on Janhvi's b'day\n",
            "headlines2:  'You shall always have my support': Arjun Kapoor pens heart-warming birthday note for Janhvi\n",
            "headlines3:  \"You will always have my support,\" Arjun Kapoor writes a heartfelt birthday note for Janhvi Kapoor.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kt-aPPiKtKlH",
        "outputId": "2aa521e1-294d-4166-a76e-7e5ba4173d86"
      },
      "source": [
        "'''\n",
        "decide threshold for min and max no-of-word-token in headline \n",
        "'''\n",
        "headlines = [headline for headline in target_headlines_1] + [headline for headline in target_headlines_2] + [headline for headline in target_headlines_3]\n",
        "headline_len=[len(headline.split(' '))for headline in headlines]\n",
        "print('5th percentile length: ',np.quantile(headline_len, 0.05))\n",
        "print('25th percentile length: ',np.quantile(headline_len, 0.25))\n",
        "print('50th percentile length: ',np.quantile(headline_len, 0.50))\n",
        "print('75th percentile length: ',np.quantile(headline_len, 0.75))\n",
        "print('95th percentile length: ',np.quantile(headline_len, 0.95))\n",
        "print('99th percentile length: ',np.quantile(headline_len, 0.99))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5th percentile length:  8.0\n",
            "25th percentile length:  10.0\n",
            "50th percentile length:  12.0\n",
            "75th percentile length:  14.0\n",
            "95th percentile length:  18.0\n",
            "99th percentile length:  22.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgR962aFuAdX",
        "outputId": "c56c8b4a-783a-4b7b-f078-96d0515f21f8"
      },
      "source": [
        "'''\n",
        "creating summary-headline pair and then randomly shuffle them\n",
        "'''\n",
        "summary_headline_pairs=list(zip(contents,target_headlines_1, target_headlines_2, target_headlines_3))\n",
        "random.shuffle(summary_headline_pairs)\n",
        "len(summary_headline_pairs)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2610"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRbh4FTduNZS"
      },
      "source": [
        "'''\n",
        "get train and test dataset\n",
        "'''\n",
        "train_summary_headline_pairs=summary_headline_pairs[0:2000]\n",
        "#train_summary_headline_pairs=summary_headline_pairs[0:100]#just for faster testing if code flow is working fine\n",
        "test_summary_headline_pairs=summary_headline_pairs[2000:]\n",
        "no_of_training_records=len(train_summary_headline_pairs)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROAdpWPb0zuP"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvQY9dOguozn",
        "outputId": "605e5e4f-f891-4284-ce7a-49849089189c"
      },
      "source": [
        "'''\n",
        "load Bert-Model and Tokeninzer using predefined weights\n",
        "distilbert-base-uncased' model is uncased: it does not make a difference between english and English. \n",
        "'''\n",
        "model_class, tokenizer_class, pretrained_weights = (transformers.BertModel, transformers.BertTokenizer, \"bert-base-uncased\")# 'distilbert-base-uncased'\n",
        "\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "model = model_class.from_pretrained(pretrained_weights)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLGesjBuu7Xo",
        "outputId": "408fab97-cd8d-4a1d-d70a-0491039e4e5b"
      },
      "source": [
        "'''\n",
        "initialize BOS and EOS token\n",
        "'''\n",
        "tokenizer.bos_token = tokenizer.cls_token\n",
        "tokenizer.eos_token = tokenizer.sep_token\n",
        "print('SOS token id: ',tokenizer.bos_token_id)\n",
        "print('EOS token id: ',tokenizer.eos_token_id)"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SOS token id:  101\n",
            "EOS token id:  102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fw8KHRYtItQq"
      },
      "source": [
        "'''\n",
        "tokenize news summary and headline\n",
        "'''\n",
        "max_encoder_len=125\n",
        "max_decoder_len=40 \n",
        "tokenized_summaries = [tokenizer(summary, padding=\"max_length\", truncation=True, max_length=max_encoder_len) for summary in contents]\n",
        "tokenized_headlines_1 = [tokenizer(target_headlines, padding=\"max_length\", truncation=True, max_length=max_decoder_len) for target_headlines in target_headlines_1]\n",
        "tokenized_headlines_2 = [tokenizer(target_headlines, padding=\"max_length\", truncation=True, max_length=max_decoder_len) for target_headlines in target_headlines_2]\n",
        "tokenized_headlines_3 = [tokenizer(target_headlines, padding=\"max_length\", truncation=True, max_length=max_decoder_len) for target_headlines in target_headlines_3]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW1d0iddJD2-"
      },
      "source": [
        "summary_lengths = [len(tokenized_summary.input_ids) for tokenized_summary in tokenized_summaries]\n",
        "headline_lengths_1 = [len(tokenized_headline.input_ids) for tokenized_headline in tokenized_headlines_1]\n",
        "headline_lengths_2 = [len(tokenized_headline.input_ids) for tokenized_headline in tokenized_headlines_2]\n",
        "headline_lengths_3 = [len(tokenized_headline.input_ids) for tokenized_headline in tokenized_headlines_3]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYb8B0pGJKai",
        "outputId": "4df98655-2ae9-4520-e4d2-ae5dc37005d9"
      },
      "source": [
        "print('summary_lengths_max: ',max(summary_lengths))\n",
        "print('headline1_lengths_max: ',max(headline_lengths_1))\n",
        "print('headline2_lengths_max: ',max(headline_lengths_2))\n",
        "print('headline3_lengths_max: ',max(headline_lengths_3))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "summary_lengths_max:  125\n",
            "headline1_lengths_max:  40\n",
            "headline2_lengths_max:  40\n",
            "headline3_lengths_max:  40\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVdMyu02u_gB"
      },
      "source": [
        "'''\n",
        "instance of class Batch_Data represent input to the encoder decoder model for a batch\n",
        "'''\n",
        "class Batch_Data:\n",
        "  def __init__(self, batch_ip_vector, batch_ip_length, batch_op_vector_1, batch_op_vector_2, batch_op_vector_3, batch_op_token_idxs_1, batch_op_token_idxs_2, \n",
        "               batch_op_token_idxs_3, batch_mask_1, batch_mask_2, batch_mask_3):\n",
        "    self.batch_ip_vector=batch_ip_vector\n",
        "    self.batch_ip_length=batch_ip_length\n",
        "    self.batch_op_vector_1=batch_op_vector_1\n",
        "    self.batch_op_vector_2=batch_op_vector_2\n",
        "    self.batch_op_vector_3=batch_op_vector_3\n",
        "    self.batch_op_token_idxs_1=batch_op_token_idxs_1\n",
        "    self.batch_op_token_idxs_2=batch_op_token_idxs_2\n",
        "    self.batch_op_token_idxs_3=batch_op_token_idxs_3\n",
        "    self.batch_mask_1=batch_mask_1\n",
        "    self.batch_mask_2=batch_mask_2\n",
        "    self.batch_mask_3=batch_mask_3"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fv7EdP5TSg1i",
        "outputId": "7d65e743-a7d2-4245-8edf-90ffb1344440"
      },
      "source": [
        "#tokenized_summaries[0].input_ids\n",
        "a=np.array([list([1,2]) for i in range(2)])\n",
        "a.shape\n",
        "tokenized_summaries_1=tokenized_summaries[2:4]\n",
        "b=np.array([list(tokenized_summary.input_ids) for tokenized_summary in tokenized_summaries_1])\n",
        "b.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 125)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfmK5SbJ34ig"
      },
      "source": [
        "'''\n",
        "rearrange numpy array based on input row index position\n",
        "'''\n",
        "def rearrange_numpy_array(np_array,pos_row_idxs):\n",
        "  return np_array[pos_row_idxs, :]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EXoatkl5RyM",
        "outputId": "f1185cd1-1a69-4ded-97f6-429486e3e50c"
      },
      "source": [
        "np_array=np.array([[1,2,3],[5,6,7],[9,10,11]])\n",
        "pos_row_idxs=[0,2,1]\n",
        "rearrange_numpy_array(np_array,pos_row_idxs)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  2,  3],\n",
              "       [ 9, 10, 11],\n",
              "       [ 5,  6,  7]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGDJoP3H6ZbZ",
        "outputId": "7c775c67-c9b5-4a53-a77c-c8b0c91c5949"
      },
      "source": [
        "ip_array=[42, 32, 41, 49, 46, 52, 19, 31, 44, 43, 38, 40, 53, 40, 48, 50]\n",
        "sorted_idxs=np.argsort(ip_array)\n",
        "print(type(sorted_idxs))\n",
        "print(type(list(sorted_idxs)))\n",
        "print('sorted_idxs: ',sorted_idxs)\n",
        "sorted_array=[]\n",
        "for indx in sorted_idxs:\n",
        "  sorted_array.append(ip_array[indx])\n",
        "print('sorted_array: ',sorted_array)\n",
        "#sorted summary_length_batch index:  [ 6  7  1 10 11 13  2  0  9  8  4 14  3 15  5 12]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "<class 'list'>\n",
            "sorted_idxs:  [ 6  7  1 10 11 13  2  0  9  8  4 14  3 15  5 12]\n",
            "sorted_array:  [19, 31, 32, 38, 40, 40, 41, 42, 43, 44, 46, 48, 49, 50, 52, 53]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5flR8xd5S792",
        "outputId": "94aa4f5d-cb13-45f3-a138-50700a63693d"
      },
      "source": [
        "'''\n",
        "run this cell only once.\n",
        "compute BERT based representation of news summary and headline and store it on drive.\n",
        "this will help in faster training, as we don't have to get bert based vector representation of headline and summary during every training iteration. \n",
        "'''\n",
        "batch_start=0\n",
        "batch_end=0\n",
        "end = 32 #len(tokenized_summaries)#32\n",
        "batch_size=16\n",
        "summary_vector=None\n",
        "\n",
        "inputs_input_ids = np.array([list(tokenized_summary.input_ids) for tokenized_summary in tokenized_summaries])\n",
        "inputs_attention_mask = np.array([np.array(tokenized_summary.attention_mask) for tokenized_summary in tokenized_summaries])\n",
        "\n",
        "outputs1_input_ids = np.array([np.array(tokenized_headline.input_ids) for tokenized_headline in tokenized_headlines_1])\n",
        "outputs1_attention_mask = np.array([np.array(tokenized_headline.attention_mask) for tokenized_headline in tokenized_headlines_1])\n",
        "\n",
        "outputs2_input_ids = np.array([np.array(tokenized_headline.input_ids) for tokenized_headline in tokenized_headlines_2])\n",
        "outputs2_attention_mask = np.array([np.array(tokenized_headline.attention_mask) for tokenized_headline in tokenized_headlines_2])\n",
        "\n",
        "outputs3_input_ids = np.array([np.array(tokenized_headline.input_ids) for tokenized_headline in tokenized_headlines_3])\n",
        "outputs3_attention_mask = np.array([np.array(tokenized_headline.attention_mask) for tokenized_headline in tokenized_headlines_3])\n",
        "\n",
        "\n",
        "while batch_end<end:\n",
        "  batch_end=batch_start+batch_size\n",
        "  if batch_end<end:\n",
        "    pass #do nothing\n",
        "  else:\n",
        "    batch_end=end\n",
        "  print('batch_start: ',batch_start,' batch_end: ',batch_end)\n",
        "  summary_batch=inputs_input_ids[batch_start:batch_end]\n",
        "  summary_length_batch=[np.count_nonzero(summary) for summary in summary_batch]\n",
        "  #print('summary_length_batch: ',summary_length_batch)\n",
        "  attention_mask_summary = inputs_attention_mask[batch_start:batch_end]\n",
        "\n",
        "  sorted_idx_pos=list(np.argsort(summary_length_batch)) #index position of input seq length based ascending order\n",
        "  sorted_idx_pos.reverse()\n",
        "\n",
        "  summary_length_batch.sort(reverse=True)\n",
        "  summary_batch=rearrange_numpy_array(summary_batch,sorted_idx_pos)\n",
        "  attention_mask_summary=rearrange_numpy_array(attention_mask_summary,sorted_idx_pos)\n",
        "\n",
        "  #print('**************summary_batch shape: ',summary_batch.shape)\n",
        "  #print('**************summary_length_batch : ',summary_length_batch)\n",
        "  #print('sorted summary_length_batch index: ',np.argsort(summary_length_batch))\n",
        "  #break\n",
        "\n",
        "  headline1_batch=outputs1_input_ids[batch_start:batch_end]\n",
        "  headline1_batch=rearrange_numpy_array(headline1_batch,sorted_idx_pos)\n",
        "  attention_mask_headline1 = outputs1_attention_mask[batch_start:batch_end]\n",
        "  attention_mask_headline1=rearrange_numpy_array(attention_mask_headline1,sorted_idx_pos)\n",
        "\n",
        "  headline2_batch=outputs2_input_ids[batch_start:batch_end]\n",
        "  headline2_batch=rearrange_numpy_array(headline2_batch,sorted_idx_pos)\n",
        "  attention_mask_headline2 = outputs2_attention_mask[batch_start:batch_end]\n",
        "  attention_mask_headline2=rearrange_numpy_array(attention_mask_headline2,sorted_idx_pos)\n",
        "\n",
        "  headline3_batch=outputs3_input_ids[batch_start:batch_end]\n",
        "  headline3_batch=rearrange_numpy_array(headline3_batch,sorted_idx_pos)\n",
        "  attention_mask_headline3 = outputs3_attention_mask[batch_start:batch_end]\n",
        "  attention_mask_headline3=rearrange_numpy_array(attention_mask_headline3,sorted_idx_pos)\n",
        "\n",
        "  summary_batch_t = torch.tensor(summary_batch) \n",
        "  attention_mask_summary_t = torch.tensor(attention_mask_summary)\n",
        "\n",
        "  headline1_batch_t = torch.tensor(headline1_batch) \n",
        "  attention_mask_headline1_t = torch.BoolTensor(attention_mask_headline1)\n",
        "  \n",
        "  headline2_batch_t = torch.tensor(headline2_batch) \n",
        "  attention_mask_headline2_t = torch.BoolTensor(attention_mask_headline2)\n",
        "\n",
        "  headline3_batch_t = torch.tensor(headline3_batch) \n",
        "  attention_mask_headline3_t = torch.BoolTensor(attention_mask_headline3)\n",
        "   \n",
        "  with torch.no_grad():\n",
        "    last_hidden_states = model(summary_batch_t, attention_mask=attention_mask_summary_t)\n",
        "  summary_batch_vector=last_hidden_states[0]\n",
        "  with torch.no_grad():\n",
        "    last_hidden_states = model(headline1_batch_t, attention_mask=attention_mask_headline1_t)\n",
        "  headline1_batch_vector=last_hidden_states[0]\n",
        "  with torch.no_grad():\n",
        "    last_hidden_states = model(headline2_batch_t, attention_mask=attention_mask_headline2_t)\n",
        "  headline2_batch_vector=last_hidden_states[0]\n",
        "  with torch.no_grad():\n",
        "    last_hidden_states = model(headline3_batch_t, attention_mask=attention_mask_headline3_t)\n",
        "  headline3_batch_vector=last_hidden_states[0]\n",
        "\n",
        "  batch_data=Batch_Data(summary_batch_vector,summary_length_batch,headline1_batch_vector,headline2_batch_vector,headline3_batch_vector,\n",
        "                        headline1_batch_t,headline2_batch_t,headline3_batch_t,attention_mask_headline1_t,attention_mask_headline2_t,attention_mask_headline3_t)\n",
        "  batch_file_path='/content/gdrive/My Drive/Capstone_Project/Data/Bert_vectors/one_to_many_setup/batch_'+str(batch_start)+'_'+str(batch_end)+'.pickle'\n",
        "  with open(batch_file_path, 'wb') as file_handle:\n",
        "    pickle.dump(batch_data, file_handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "  batch_start=batch_end"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch_start:  0  batch_end:  16\n",
            "summary_length_batch:  [83, 93, 84, 76, 79, 73, 106, 94, 81, 82, 87, 85, 72, 85, 77, 75]\n",
            "batch_start:  16  batch_end:  32\n",
            "summary_length_batch:  [92, 88, 88, 103, 102, 102, 94, 85, 87, 66, 83, 73, 94, 99, 80, 70]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFnEzNC0wHtQ"
      },
      "source": [
        "'''\n",
        "function for providing emebded representation of BOS token at first timestep of decoding for complete batch\n",
        "'''\n",
        "def get_initial_decoder_ip(batch_size):\n",
        "  sos_token_tensor=torch.tensor([[tokenizer.bos_token_id]])\n",
        "  with torch.no_grad():\n",
        "    last_hidden_states = model(sos_token_tensor)\n",
        "  SOS_token_bert_vector=last_hidden_states[0]\n",
        "  SOS_token_bert_vector=torch.squeeze(SOS_token_bert_vector, 0)\n",
        "  decoder_input = torch.tensor([SOS_token_bert_vector.numpy() for _ in range(batch_size)])\n",
        "  print('decoder_input shape: ',decoder_input.shape)\n",
        "  decoder_input=decoder_input.permute(1,0,2)\n",
        "  print('decoder_input shape: ',decoder_input.shape)\n",
        "  return decoder_input"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izZUK-y819gJ",
        "outputId": "c722572b-49d8-43e4-9808-8bfa451f4fa2"
      },
      "source": [
        "'''\n",
        "just for testing\n",
        "'''\n",
        "get_initial_decoder_ip(32)"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "decoder_input shape:  torch.Size([32, 1, 768])\n",
            "decoder_input shape:  torch.Size([1, 32, 768])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.7868,  0.3158,  0.1873,  ...,  0.1196,  0.4806,  0.2568],\n",
              "         [-0.7868,  0.3158,  0.1873,  ...,  0.1196,  0.4806,  0.2568],\n",
              "         [-0.7868,  0.3158,  0.1873,  ...,  0.1196,  0.4806,  0.2568],\n",
              "         ...,\n",
              "         [-0.7868,  0.3158,  0.1873,  ...,  0.1196,  0.4806,  0.2568],\n",
              "         [-0.7868,  0.3158,  0.1873,  ...,  0.1196,  0.4806,  0.2568],\n",
              "         [-0.7868,  0.3158,  0.1873,  ...,  0.1196,  0.4806,  0.2568]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxAlnCsiJvgJ"
      },
      "source": [
        "'''\n",
        "utility function for creating batch from stored BERT based news headline and summary vector \n",
        "'''\n",
        "def prepare_batches():\n",
        "  training_batch_location='/content/gdrive/My Drive/Capstone_Project/Data/Bert_vectors/one_to_many_setup/*'\n",
        "  batch_files=glob.glob(training_batch_location)\n",
        "  random.shuffle(batch_files)\n",
        "  #print(batch_files)\n",
        "  return batch_files \n",
        "\n",
        "def get_data_for_current_iteration(iteration_index,batch_files,batch_size=16):\n",
        "  no_of_files_per_batch=int(batch_size/16) #every batch file on disk has 16 training records and batch size shalll be multiple of 16\n",
        "  start_idx=iteration_index*no_of_files_per_batch\n",
        "  end_idx=start_idx+no_of_files_per_batch\n",
        "  files_for_this_iter=batch_files[start_idx:end_idx]#end_idx is exclusive and start_idx is inclusive\n",
        "  print('files_for_this_iter: ',files_for_this_iter)\n",
        "  batch_data=None\n",
        "  for file in files_for_this_iter:\n",
        "    #print('for loop')\n",
        "    with open(file, 'rb') as file_handle:\n",
        "      batch_data_loaded=pickle.load(file_handle)  \n",
        "      #print('*************1')\n",
        "    if (batch_data==None):\n",
        "      #print('*************2')\n",
        "      batch_data=batch_data_loaded\n",
        "      #print('*************3')\n",
        "    else:\n",
        "      #print('*************4')\n",
        "      \n",
        "      batch_ip_vector=torch.vstack((batch_data.batch_ip_vector,batch_data_loaded.batch_ip_vector))\n",
        "      batch_ip_length=batch_data.batch_ip_length + batch_data_loaded.batch_ip_length\n",
        "      #\n",
        "      batch_op_vector_1=torch.vstack((batch_data.batch_op_vector_1,batch_data_loaded.batch_op_vector_1))\n",
        "      batch_op_token_idxs_1=torch.vstack((batch_data.batch_op_token_idxs_1,batch_data_loaded.batch_op_token_idxs_1))\n",
        "      batch_mask_1=torch.vstack((batch_data.batch_mask_1,batch_data_loaded.batch_mask_1))\n",
        "      #\n",
        "      batch_op_vector_2=torch.vstack((batch_data.batch_op_vector_2,batch_data_loaded.batch_op_vector_2))\n",
        "      batch_op_token_idxs_2=torch.vstack((batch_data.batch_op_token_idxs_2,batch_data_loaded.batch_op_token_idxs_2))\n",
        "      batch_mask_2=torch.vstack((batch_data.batch_mask_2,batch_data_loaded.batch_mask_2))\n",
        "      #\n",
        "      batch_op_vector_3=torch.vstack((batch_data.batch_op_vector_3,batch_data_loaded.batch_op_vector_3))\n",
        "      batch_op_token_idxs_3=torch.vstack((batch_data.batch_op_token_idxs_3,batch_data_loaded.batch_op_token_idxs_3))\n",
        "      batch_mask_3=torch.vstack((batch_data.batch_mask_3,batch_data_loaded.batch_mask_3))\n",
        "      #\n",
        "      batch_data=Batch_Data(batch_ip_vector,batch_ip_length, batch_op_vector_1, batch_op_vector_2, batch_op_vector_3,batch_op_token_idxs_1, batch_op_token_idxs_2, \n",
        "               batch_op_token_idxs_3, batch_mask_1, batch_mask_2, batch_mask_3)\n",
        "  #print('*************5')\n",
        "  return batch_data"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEjzLm9cNs3_"
      },
      "source": [
        "'''\n",
        "test function for ip data generation for a given training iteration\n",
        "'''\n",
        "def trainIters_test():\n",
        "  num_sample=32 #TODO need to be initialized prperly\n",
        "  batch_size=16 #TODO need to be initialized prperly\n",
        "  n_epoch=2 #TODO need to be initialized prperly\n",
        "  max_target_len=44 #TODO need to be initialized prperly\n",
        "  num_iteration=int(num_sample/batch_size)\n",
        "  for epoch in range(n_epoch):\n",
        "    print('epoch is in progress: ',epoch+1)  \n",
        "    batch_files=prepare_batches()\n",
        "    training_batches =[] \n",
        "    for iteration_index in range(num_iteration):\n",
        "      print('*********************iteration index:',str(iteration_index),'*********************')\n",
        "      # Run a training iteration with batch\n",
        "      # Extract fields from batch\n",
        "      batch_data=get_data_for_current_iteration(iteration_index,batch_files,batch_size)\n",
        "      input_variable,lengths=batch_data.batch_ip_vector,batch_data.batch_ip_length\n",
        "      target_variable_1,target_variable_2,target_variable_3=batch_data.batch_op_vector_1,batch_data.batch_op_vector_2,batch_data.batch_op_vector_3\n",
        "      batch_op_token_idxs_1,batch_op_token_idxs_2,batch_op_token_idxs_3=batch_data.batch_op_token_idxs_1,batch_data.batch_op_token_idxs_2,batch_data.batch_op_token_idxs_3\n",
        "      mask_1,mask_2,mask_3=batch_data.batch_mask_1,batch_data.batch_mask_2,batch_data.batch_mask_3\n",
        "\n",
        "      input_variable=input_variable.permute(1,0,2)\n",
        "      lengths = torch.tensor(lengths)\n",
        "      print('input_variable: ',input_variable.shape, \"type: \",type(input_variable))\n",
        "      print('batch_ip_length: ',lengths.shape, \"type: \",type(lengths))\n",
        "\n",
        "      print('target_variable1: ',target_variable_1.shape, \"type: \",type(target_variable_1))\n",
        "      print('target_variable2: ',target_variable_2.shape, \"type: \",type(target_variable_2))\n",
        "      print('target_variable3: ',target_variable_3.shape, \"type: \",type(target_variable_3))\n",
        "\n",
        "      print('mask1: ',mask_1.shape, \"type: \",type(mask_1))\n",
        "      print('mask2: ',mask_2.shape, \"type: \",type(mask_2))\n",
        "      print('mask3: ',mask_3.shape, \"type: \",type(mask_3))\n",
        "      \n",
        "      "
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92nChbBrPMkK",
        "outputId": "8aafd226-1544-4fee-bfd9-e93f7f370497"
      },
      "source": [
        "'''\n",
        "test ip data generation for a given training iteration\n",
        "'''\n",
        "trainIters_test()"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch is in progress:  1\n",
            "*********************iteration index: 0 *********************\n",
            "files_for_this_iter:  ['/content/gdrive/My Drive/Capstone_Project/Data/Bert_vectors/one_to_many_setup/batch_0_16.pickle']\n",
            "input_variable:  torch.Size([125, 16, 768]) type:  <class 'torch.Tensor'>\n",
            "batch_ip_length:  torch.Size([16]) type:  <class 'torch.Tensor'>\n",
            "target_variable1:  torch.Size([16, 40, 768]) type:  <class 'torch.Tensor'>\n",
            "target_variable2:  torch.Size([16, 40, 768]) type:  <class 'torch.Tensor'>\n",
            "target_variable3:  torch.Size([16, 40, 768]) type:  <class 'torch.Tensor'>\n",
            "mask1:  torch.Size([16, 40]) type:  <class 'torch.Tensor'>\n",
            "mask2:  torch.Size([16, 40]) type:  <class 'torch.Tensor'>\n",
            "mask3:  torch.Size([16, 40]) type:  <class 'torch.Tensor'>\n",
            "*********************iteration index: 1 *********************\n",
            "files_for_this_iter:  ['/content/gdrive/My Drive/Capstone_Project/Data/Bert_vectors/one_to_many_setup/batch_16_32.pickle']\n",
            "input_variable:  torch.Size([125, 16, 768]) type:  <class 'torch.Tensor'>\n",
            "batch_ip_length:  torch.Size([16]) type:  <class 'torch.Tensor'>\n",
            "target_variable1:  torch.Size([16, 40, 768]) type:  <class 'torch.Tensor'>\n",
            "target_variable2:  torch.Size([16, 40, 768]) type:  <class 'torch.Tensor'>\n",
            "target_variable3:  torch.Size([16, 40, 768]) type:  <class 'torch.Tensor'>\n",
            "mask1:  torch.Size([16, 40]) type:  <class 'torch.Tensor'>\n",
            "mask2:  torch.Size([16, 40]) type:  <class 'torch.Tensor'>\n",
            "mask3:  torch.Size([16, 40]) type:  <class 'torch.Tensor'>\n",
            "epoch is in progress:  2\n",
            "*********************iteration index: 0 *********************\n",
            "files_for_this_iter:  ['/content/gdrive/My Drive/Capstone_Project/Data/Bert_vectors/one_to_many_setup/batch_16_32.pickle']\n",
            "input_variable:  torch.Size([125, 16, 768]) type:  <class 'torch.Tensor'>\n",
            "batch_ip_length:  torch.Size([16]) type:  <class 'torch.Tensor'>\n",
            "target_variable1:  torch.Size([16, 40, 768]) type:  <class 'torch.Tensor'>\n",
            "target_variable2:  torch.Size([16, 40, 768]) type:  <class 'torch.Tensor'>\n",
            "target_variable3:  torch.Size([16, 40, 768]) type:  <class 'torch.Tensor'>\n",
            "mask1:  torch.Size([16, 40]) type:  <class 'torch.Tensor'>\n",
            "mask2:  torch.Size([16, 40]) type:  <class 'torch.Tensor'>\n",
            "mask3:  torch.Size([16, 40]) type:  <class 'torch.Tensor'>\n",
            "*********************iteration index: 1 *********************\n",
            "files_for_this_iter:  ['/content/gdrive/My Drive/Capstone_Project/Data/Bert_vectors/one_to_many_setup/batch_0_16.pickle']\n",
            "input_variable:  torch.Size([125, 16, 768]) type:  <class 'torch.Tensor'>\n",
            "batch_ip_length:  torch.Size([16]) type:  <class 'torch.Tensor'>\n",
            "target_variable1:  torch.Size([16, 40, 768]) type:  <class 'torch.Tensor'>\n",
            "target_variable2:  torch.Size([16, 40, 768]) type:  <class 'torch.Tensor'>\n",
            "target_variable3:  torch.Size([16, 40, 768]) type:  <class 'torch.Tensor'>\n",
            "mask1:  torch.Size([16, 40]) type:  <class 'torch.Tensor'>\n",
            "mask2:  torch.Size([16, 40]) type:  <class 'torch.Tensor'>\n",
            "mask3:  torch.Size([16, 40]) type:  <class 'torch.Tensor'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiB9tB7ldjJ4"
      },
      "source": [
        "'''\n",
        "GRU based encoder class without any embedding layer (as input will precomputed bert vector representation of news data)\n",
        "'''\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, embbed_dim, hidden_dim, num_layers):\n",
        "       super(Encoder, self).__init__()\n",
        "       #set the encoder input dimesion , embbed dimesion, hidden dimesion, and number of layers \n",
        "       self.hidden_dim = hidden_dim\n",
        "       self.num_layers = num_layers\n",
        "       self.embbed_dim=embbed_dim\n",
        "       #intialize the GRU to take the input dimetion of embbed, and output dimention of hidden and\n",
        "       #set the number of gru layers\n",
        "       self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers)\n",
        "\n",
        "  def forward(self, input_seq, input_lengths, hidden=None):\n",
        "    print('inside encoder forward function || input_seq shape: ',input_seq.shape )\n",
        "    print('inside encoder forward function || input_lengths shape: ',input_lengths.shape )\n",
        "    if(hidden!=None):\n",
        "      torch.set_printoptions(threshold=10000)\n",
        "      print('inside encoder forward function || hidden shape: ',hidden )\n",
        "    # Pack padded batch of sequences for RNN module\n",
        "    \n",
        "    # Forward pass through GRU\n",
        "    outputs, hidden = self.gru(input_seq, hidden)\n",
        "    '''\n",
        "    packed = nn.utils.rnn.pack_padded_sequence(input_seq, input_lengths)\n",
        "    outputs, hidden = self.gru(packed, hidden)##TODO \n",
        "    outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "    '''\n",
        "    # Unpack padding\n",
        "    \n",
        "    #print('inside encoder forward function || outputs shape: ',outputs.shape )\n",
        "    #print('inside encoder forward function || hidden shape: ',hidden )\n",
        "    #print('inside encoder forward function || outputs[:, : ,self.hidden_dim:] shape: ',outputs[:, : ,self.hidden_dim:].shape )\n",
        "    # Sum bidirectional GRU outputs\n",
        "    #outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
        "    # Return output and final hidden state\n",
        "    print('inside encoder forward function || outputs shape: ',outputs.shape )\n",
        "    print('inside encoder forward function || hidden shape: ',hidden.shape )\n",
        "    return outputs, hidden   "
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-M3nsDKvdkAf"
      },
      "source": [
        "'''\n",
        "Luong attention layer\n",
        "'''\n",
        "class Attn(nn.Module):\n",
        "    def __init__(self, method, hidden_size):\n",
        "        super(Attn, self).__init__()\n",
        "        self.method = method\n",
        "        if self.method not in ['dot', 'general', 'concat']:\n",
        "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
        "        self.hidden_size = hidden_size\n",
        "        if self.method == 'general':\n",
        "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
        "        elif self.method == 'concat':\n",
        "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
        "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
        "\n",
        "    def dot_score(self, hidden, encoder_output):\n",
        "        return torch.sum(hidden * encoder_output, dim=2)\n",
        "\n",
        "    def general_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(encoder_output)\n",
        "        return torch.sum(hidden * energy, dim=2)\n",
        "\n",
        "    def concat_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
        "        return torch.sum(self.v * energy, dim=2)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # Calculate the attention weights (energies) based on the given method\n",
        "        if self.method == 'general':\n",
        "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'concat':\n",
        "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'dot':\n",
        "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
        "\n",
        "        # Transpose max_length and batch_size dimensions\n",
        "        attn_energies = attn_energies.t()\n",
        "\n",
        "        # Return the softmax normalized probability scores (with added dimension)\n",
        "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhcaalQLdmzh"
      },
      "source": [
        "'''\n",
        "GRU with Luong Attn based Decoder class \n",
        "'''\n",
        "class LuongAttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, attn_model, embbed_dim, hidden_size, output_size, num_layers=1 ):\n",
        "        super(LuongAttnDecoderRNN, self).__init__()\n",
        "\n",
        "        # Keep for reference\n",
        "        self.embbed_dim=embbed_dim\n",
        "        self.attn_model = attn_model\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Define layers\n",
        "        self.gru = nn.GRU(self.embbed_dim, self.hidden_size, self.num_layers)\n",
        "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.attn = Attn(attn_model, hidden_size)\n",
        "\n",
        "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
        "        # Note: we run this one step (word) at a time\n",
        "        #print(\"decoder forward 1 embedded shape: \",embedded.shape)\n",
        "        #embedded = self.embedding_dropout(embedded)\n",
        "        # Forward through unidirectional GRU\n",
        "        rnn_output, hidden = self.gru(input_step, last_hidden)\n",
        "        #print(\"decoder forward 2 rnn_output shape: \",rnn_output.shape)\n",
        "        #print(\"decoder forward 2 hidden shape: \",hidden.shape)\n",
        "        # Calculate attention weights from the current GRU output\n",
        "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
        "        #print(\"decoder forward 3 attn_weights shape: \",attn_weights.shape)\n",
        "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
        "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
        "        #print(\"decoder forward 4 context shape: \",context.shape)\n",
        "        # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
        "        rnn_output = rnn_output.squeeze(0)\n",
        "        context = context.squeeze(1)\n",
        "        #print(\"decoder forward 5 rnn_output.squeeze shape: \",rnn_output.shape)\n",
        "        #print(\"decoder forward 5 context.squeeze shape: \",context.shape)\n",
        "        concat_input = torch.cat((rnn_output, context), 1)\n",
        "        concat_output = torch.tanh(self.concat(concat_input))\n",
        "        #print(\"decoder forward 6 concat_input shape: \",concat_input.shape)\n",
        "        #print(\"decoder forward 6 concat_output shape: \",concat_output.shape)        \n",
        "        # Predict next word using Luong eq. 6\n",
        "        output = self.out(concat_output)\n",
        "        #print(\"decoder forward 7 output shape: \",output.shape)\n",
        "        output = F.softmax(output, dim=1)\n",
        "        #print(\"decoder forward 8 output shape: \",output.shape)\n",
        "        # {Return word2 output} {and output} {output} and final hidden state\n",
        "        return output, hidden"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1yOxViYdrYH"
      },
      "source": [
        "'''\n",
        "loss function that calculates the average negative log likelihood of the elements that correspond to a 1 in the mask tensor\n",
        "'''\n",
        "def maskNLLLoss(inp, target, mask):\n",
        "    nTotal = mask.sum()\n",
        "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
        "    loss = crossEntropy.masked_select(mask).mean()\n",
        "    loss = loss.to(device)\n",
        "    return loss, nTotal.item()"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "224Nvav01i8C"
      },
      "source": [
        "'''\n",
        "function for performing a single training iteration\n",
        "'''\n",
        "def train(input_variable, lengths, target_variable1, target_variable2, target_variable3, target_op_token_idxs1, target_op_token_idxs2, target_op_token_idxs3, mask1, mask2,mask3, \n",
        "          max_target_len, encoder, decoder1, decoder2, decoder3,\n",
        "          encoder_optimizer, decoder_optimizer_1, decoder_optimizer_2, decoder_optimizer_3, batch_size, clip, decoder_ip_initial, teacher_forcing_ratio=1):\n",
        "    # Zero gradients             \n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer_1.zero_grad()\n",
        "    decoder_optimizer_2.zero_grad()\n",
        "    decoder_optimizer_3.zero_grad()\n",
        "\n",
        "    # Set device options\n",
        "    input_variable = input_variable.to(device)\n",
        "    target_variable1 = target_variable1.to(device)\n",
        "    target_variable2 = target_variable2.to(device)\n",
        "    target_variable3 = target_variable3.to(device)\n",
        "    mask1 = mask1.to(device)\n",
        "    mask2 = mask2.to(device)\n",
        "    mask3 = mask3.to(device)\n",
        "\n",
        "    # Lengths for rnn packing should always be on the cpu\n",
        "    lengths = lengths.to(\"cpu\")\n",
        "\n",
        "    # Initialize variables\n",
        "    loss = 0\n",
        "    print_losses = []\n",
        "    n_totals = 0\n",
        "\n",
        "    # Forward pass through encoder\n",
        "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
        "\n",
        "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
        "    print('inside train function decoder_ip_initial shape: ',decoder_ip_initial.shape)\n",
        "    decoder_input = decoder_ip_initial#sos token for all the training sample in a given batch\n",
        "    decoder1_input = decoder2_input = decoder3_input = decoder_input.to(device)\n",
        "\n",
        "    # Set initial decoder hidden state to the encoder's final hidden state\n",
        "    decoder1_hidden = decoder2_hidden = decoder3_hidden = encoder_hidden[:decoder1.num_layers] #this important and handle scenario where no of layer for GRU varries in encoder decoder\n",
        "    \n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False #TODO need to work on this\n",
        "    \n",
        "    max_target_len=10 # TODO we can get this value from actual op seq \n",
        "    \n",
        "   \n",
        "    if use_teacher_forcing:\n",
        "        #iterate through timesteps for decoder \n",
        "        print('teacher forcing will be used for this batch')\n",
        "        for timestep in range(1,max_target_len):            \n",
        "            #print('before decoder forward pass: ')\n",
        "            #print('decoder_input shape: ',decoder_input.shape)\n",
        "            #print('decoder_hidden shape: ',decoder_hidden.shape)\n",
        "            #print('encoder_outputs shape: ',encoder_outputs.shape)\n",
        "            print('timestep: ',timestep,' inside train function1: ',decoder_input.shape)\n",
        "            decoder1_output, decoder1_hidden = decoder1(decoder1_input, decoder1_hidden, encoder_outputs) #(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder2_output, decoder2_hidden = decoder2(decoder2_input, decoder2_hidden, encoder_outputs)\n",
        "            decoder3_output, decoder3_hidden = decoder3(decoder3_input, decoder3_hidden, encoder_outputs)\n",
        "\n",
        "            # Teacher forcing: next input is current target\n",
        "            # print('timestep: ',timestep,' inside train function2: ',decoder_input.shape)\n",
        "            decoder1_input = torch.unsqueeze(target_variable1[timestep],0)#[1,64] use next timestamp token from target seq  as ip to decoder at nexe time step\n",
        "            decoder2_input = torch.unsqueeze(target_variable2[timestep],0)\n",
        "            decoder3_input = torch.unsqueeze(target_variable3[timestep],0)\n",
        "\n",
        "            #print('target_variable[timestep] shape: ',target_variable[timestep].shape)#[64]\n",
        "            #print('target_variable[timestep].view(1, -1) shape: ',target_variable[timestep].view(1, -1).shape)#[1,64]\n",
        "            # Calculate and accumulate loss\n",
        "            mask_loss1, nTotal1 = maskNLLLoss(decoder1_output, target_op_token_idxs1[timestep], mask1[timestep])\n",
        "            mask_loss2, nTotal2 = maskNLLLoss(decoder2_output, target_op_token_idxs2[timestep], mask2[timestep])\n",
        "            mask_loss3, nTotal3 = maskNLLLoss(decoder3_output, target_op_token_idxs3[timestep], mask3[timestep])\n",
        "\n",
        "            mask_loss=mask_loss1 + mask_loss2 + mask_loss3\n",
        "            nTotal=nTotal1+nTotal2+nTotal3\n",
        "            loss += mask_loss\n",
        "            #print('mask_loss1: ',mask_loss1)\n",
        "            #print('mask_loss2: ',mask_loss2)\n",
        "            #print('mask_loss3: ',mask_loss3)\n",
        "            #print('type of mask_loss1: ',type(mask_loss1))\n",
        "            #print('timestep : ',timestep,' mask_loss: ',mask_loss,' loss: ',loss)\n",
        "            \n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "            non_padding_token_count=(mask1[timestep+1].sum()+mask1[timestep+1].sum()+mask1[timestep+1].sum())#non_padding_token_count for next timestep\n",
        "            #print('non_padding_token_count: ',non_padding_token_count)\n",
        "            if(non_padding_token_count==0):#all tokens are padding token for next timestep for all records in batches \n",
        "              break\n",
        "    else:\n",
        "        print('teacher forcing won\\'t be used for this batch')\n",
        "        for timestep in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            # No teacher forcing: next input is decoder's own current output\n",
        "            _, topi = decoder_output.topk(1)\n",
        "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
        "            decoder_input = decoder_input.to(device)\n",
        "            # Calculate and accumulate loss\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "\n",
        "    # Perform backpropatation\n",
        "    print('before calling backward pass')\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip gradients: gradients are modified in place\n",
        "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "    _ = nn.utils.clip_grad_norm_(decoder1.parameters(), clip)\n",
        "    _ = nn.utils.clip_grad_norm_(decoder2.parameters(), clip)\n",
        "    _ = nn.utils.clip_grad_norm_(decoder3.parameters(), clip)\n",
        "\n",
        "    # Adjust model weights\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer_1.step()\n",
        "    decoder_optimizer_2.step()\n",
        "    decoder_optimizer_3.step()\n",
        "\n",
        "    return sum(print_losses) / n_totals"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiMwrVdM1vA0"
      },
      "source": [
        "'''\n",
        "function responsible for running n epoch of training given the passed models, optimizers, data etc.\n",
        "'''\n",
        "def trainIters(n_epoch, batch_size, num_sample, max_target_len, encoder, decoder1, decoder2, decoder3,\n",
        "          encoder_optimizer, decoder_optimizer_1, decoder_optimizer_2, decoder_optimizer_3,  decoder_ip_initial, print_every, clip):\n",
        "  num_iteration=int(num_sample/batch_size)    \n",
        "  num_iteration=2 #TODO need to commented just for testing if complete training code is working fine\n",
        "  for epoch in range(n_epoch):\n",
        "    print('epoch is in progress: ',epoch+1)  \n",
        "    batch_files=prepare_batches()\n",
        "    print_loss=0\n",
        "    for iteration_index in range(num_iteration):\n",
        "      print('*********************iteration index:',str(iteration_index),'*********************')\n",
        "      # Run a training iteration with batch\n",
        "      # Extract fields from batch\n",
        "      batch_data=get_data_for_current_iteration(iteration_index,batch_files,batch_size)\n",
        "      print('1****************')\n",
        "      input_variable,lengths=batch_data.batch_ip_vector,batch_data.batch_ip_length\n",
        "      target_variable_1,target_variable_2,target_variable_3=batch_data.batch_op_vector_1,batch_data.batch_op_vector_2,batch_data.batch_op_vector_3\n",
        "      target_op_token_idxs_1,target_op_token_idxs_2,target_op_token_idxs_3=batch_data.batch_op_token_idxs_1,batch_data.batch_op_token_idxs_2,batch_data.batch_op_token_idxs_3\n",
        "      mask_1,mask_2,mask_3=batch_data.batch_mask_1,batch_data.batch_mask_2,batch_data.batch_mask_3\n",
        "\n",
        "      input_variable=input_variable.permute(1,0,2)\n",
        "      lengths = torch.tensor(lengths)\n",
        "      print('input_variable shape: ',input_variable.shape)\n",
        "      print('lengths: ',lengths)\n",
        "\n",
        "      target_variable_1=target_variable_1.permute(1,0,2)\n",
        "      target_variable_2=target_variable_2.permute(1,0,2)\n",
        "      target_variable_3=target_variable_3.permute(1,0,2)\n",
        "      print('target_variable shape: ',(target_variable_1.shape, target_variable_2.shape, target_variable_3.shape))\n",
        "\n",
        "\n",
        "      target_op_token_idxs_1=target_op_token_idxs_1.permute(1,0)\n",
        "      target_op_token_idxs_2=target_op_token_idxs_2.permute(1,0)\n",
        "      target_op_token_idxs_3=target_op_token_idxs_3.permute(1,0)\n",
        "      print('target_op_token_idxs shape: ',(target_op_token_idxs_1.shape, target_op_token_idxs_2.shape, target_op_token_idxs_3.shape))\n",
        "\n",
        "      mask_1=mask_1.permute(1,0)\n",
        "      mask_2=mask_2.permute(1,0)\n",
        "      mask_3=mask_3.permute(1,0)\n",
        "      print('mask_1 shape: ',(mask_1.shape, mask_2.shape, mask_3.shape))\n",
        "\n",
        "      #print('input_variable: ',input_variable.shape, \"type: \",type(input_variable))\n",
        "      #print('target_variable: ',target_variable.shape, \"type: \",type(target_variable))\n",
        "      #print('target_op_token_idxs: ',target_op_token_idxs.shape, \"type: \",type(target_op_token_idxs))\n",
        "      #torch.set_printoptions(profile=\"full\")\n",
        "      #print('target_op_token_idxs: ',target_op_token_idxs)\n",
        "      #torch.set_printoptions(profile=\"default\") # reset#\n",
        "      #break\n",
        "      #print('mask: ',mask.shape, \"type: \",type(mask))\n",
        "      #print('batch_ip_length: ',lengths.shape, \"type: \",type(lengths))\n",
        "      \n",
        "      print('inside trainIters before calling train')\n",
        "      loss = train(input_variable, lengths, target_variable_1, target_variable_2, target_variable_3, target_op_token_idxs_1, target_op_token_idxs_2, target_op_token_idxs_3, mask_1, mask_2, mask_3, \n",
        "          max_target_len, encoder, decoder1, decoder2, decoder3,\n",
        "          encoder_optimizer, decoder_optimizer_1, decoder_optimizer_2, decoder_optimizer_3, batch_size, clip, decoder_ip_initial, teacher_forcing_ratio=1)\n",
        "\n",
        "      print('inside trainIters after calling train')\n",
        "      print_loss += loss\n",
        "      print_every=1 #print after every 10 iteration\n",
        "      # Print training progress\n",
        "      if iteration_index % print_every == 0: \n",
        "        print_loss_avg = print_loss / print_every\n",
        "        print('Epoch: ',epoch+1,' Iteration: ',iteration_index,' avg_loss: ',print_loss_avg)\n",
        "        print_loss = 0"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsKyKOru1wl2",
        "outputId": "fc60a5c4-a256-46d9-d03f-9365ada88a5f"
      },
      "source": [
        "'''\n",
        "Configure training and optimization parameter\n",
        "'''\n",
        "encoder_n_layers=1 \n",
        "decoder_n_layers=1\n",
        "\n",
        "clip = 50.0\n",
        "teacher_forcing_ratio = 1.0\n",
        "learning_rate = 0.0001\n",
        "decoder_learning_ratio = 5.0\n",
        "n_epoch = 1 #10\n",
        "print_every = 10\n",
        "save_every = 500\n",
        "\n",
        "##\n",
        "num_sample = 32 #padded_summaries.shape\n",
        "max_target_len = 40 #headline_max_len\n",
        "\n",
        "\n",
        "##\n",
        "vocab_len=30522 #\n",
        "output_size=30521 # BERT vocab_size  30522\n",
        "embed_size = 768\n",
        "hidden_size = 256\n",
        "batch_size = 16\n",
        "#num_iteration = 100000\n",
        "\n",
        "\n",
        "# Initialize encoder & decoder models\n",
        "encoder = Encoder(embed_size, hidden_size, encoder_n_layers)\n",
        "attn_model = 'dot'\n",
        "decoder_1 = LuongAttnDecoderRNN(attn_model, embed_size, hidden_size, output_size, decoder_n_layers)\n",
        "decoder_2 = LuongAttnDecoderRNN(attn_model, embed_size, hidden_size, output_size, decoder_n_layers)\n",
        "decoder_3 = LuongAttnDecoderRNN(attn_model, embed_size, hidden_size, output_size, decoder_n_layers)\n",
        "\n",
        "# Ensure dropout layers are in train mode\n",
        "encoder.train()\n",
        "decoder_1.train()\n",
        "decoder_2.train()\n",
        "decoder_3.train()\n",
        "\n",
        "# Initialize optimizers\n",
        "print('Building optimizers ...')\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer_1 = optim.Adam(decoder_1.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "decoder_optimizer_2 = optim.Adam(decoder_2.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "decoder_optimizer_3 = optim.Adam(decoder_3.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "\n",
        "decoder_ip_initial=get_initial_decoder_ip(batch_size)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building optimizers ...\n",
            "decoder_input shape:  torch.Size([16, 1, 768])\n",
            "decoder_input shape:  torch.Size([1, 16, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-UpwBgs1win",
        "outputId": "cb4ab130-7b5d-45f8-f00f-48d62346654b"
      },
      "source": [
        "'''\n",
        "train the model for specified no of epoch\n",
        "'''\n",
        "trainIters(n_epoch, batch_size, num_sample, max_target_len, encoder, decoder_1, decoder_2, \n",
        "           decoder_3, encoder_optimizer, decoder_optimizer_1, decoder_optimizer_2, decoder_optimizer_3,  decoder_ip_initial, print_every, clip)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch is in progress:  1\n",
            "*********************iteration index: 0 *********************\n",
            "files_for_this_iter:  ['/content/gdrive/My Drive/Capstone_Project/Data/Bert_vectors/one_to_many_setup/batch_16_32.pickle']\n",
            "1****************\n",
            "input_variable shape:  torch.Size([125, 16, 768])\n",
            "lengths:  tensor([103, 102, 102,  99,  94,  94,  92,  88,  88,  87,  85,  83,  80,  73,\n",
            "         70,  66])\n",
            "target_variable shape:  (torch.Size([40, 16, 768]), torch.Size([40, 16, 768]), torch.Size([40, 16, 768]))\n",
            "target_op_token_idxs shape:  (torch.Size([40, 16]), torch.Size([40, 16]), torch.Size([40, 16]))\n",
            "mask_1 shape:  (torch.Size([40, 16]), torch.Size([40, 16]), torch.Size([40, 16]))\n",
            "inside trainIters before calling train\n",
            "inside encoder forward function || input_seq shape:  torch.Size([125, 16, 768])\n",
            "inside encoder forward function || input_lengths shape:  torch.Size([16])\n",
            "inside encoder forward function || outputs shape:  torch.Size([125, 16, 256])\n",
            "inside encoder forward function || hidden shape:  torch.Size([1, 16, 256])\n",
            "inside train function decoder_ip_initial shape:  torch.Size([1, 16, 768])\n",
            "teacher forcing will be used for this batch\n",
            "timestep:  1  inside train function1:  torch.Size([1, 16, 768])\n",
            "timestep:  2  inside train function1:  torch.Size([1, 16, 768])\n",
            "timestep:  3  inside train function1:  torch.Size([1, 16, 768])\n",
            "timestep:  4  inside train function1:  torch.Size([1, 16, 768])\n",
            "timestep:  5  inside train function1:  torch.Size([1, 16, 768])\n",
            "timestep:  6  inside train function1:  torch.Size([1, 16, 768])\n",
            "timestep:  7  inside train function1:  torch.Size([1, 16, 768])\n",
            "timestep:  8  inside train function1:  torch.Size([1, 16, 768])\n",
            "timestep:  9  inside train function1:  torch.Size([1, 16, 768])\n",
            "before calling backward pass\n",
            "inside trainIters after calling train\n",
            "Epoch:  1  Iteration:  0  avg_loss:  31.008987214830185\n",
            "*********************iteration index: 1 *********************\n",
            "files_for_this_iter:  ['/content/gdrive/My Drive/Capstone_Project/Data/Bert_vectors/one_to_many_setup/batch_0_16.pickle']\n",
            "1****************\n",
            "input_variable shape:  torch.Size([125, 16, 768])\n",
            "lengths:  tensor([106,  94,  93,  87,  85,  85,  84,  83,  82,  81,  79,  77,  76,  75,\n",
            "         73,  72])\n",
            "target_variable shape:  (torch.Size([40, 16, 768]), torch.Size([40, 16, 768]), torch.Size([40, 16, 768]))\n",
            "target_op_token_idxs shape:  (torch.Size([40, 16]), torch.Size([40, 16]), torch.Size([40, 16]))\n",
            "mask_1 shape:  (torch.Size([40, 16]), torch.Size([40, 16]), torch.Size([40, 16]))\n",
            "inside trainIters before calling train\n",
            "inside encoder forward function || input_seq shape:  torch.Size([125, 16, 768])\n",
            "inside encoder forward function || input_lengths shape:  torch.Size([16])\n",
            "inside encoder forward function || outputs shape:  torch.Size([125, 16, 256])\n",
            "inside encoder forward function || hidden shape:  torch.Size([1, 16, 256])\n",
            "inside train function decoder_ip_initial shape:  torch.Size([1, 16, 768])\n",
            "teacher forcing will be used for this batch\n",
            "timestep:  1  inside train function1:  torch.Size([1, 16, 768])\n",
            "timestep:  2  inside train function1:  torch.Size([1, 16, 768])\n",
            "timestep:  3  inside train function1:  torch.Size([1, 16, 768])\n",
            "timestep:  4  inside train function1:  torch.Size([1, 16, 768])\n",
            "timestep:  5  inside train function1:  torch.Size([1, 16, 768])\n",
            "timestep:  6  inside train function1:  torch.Size([1, 16, 768])\n",
            "timestep:  7  inside train function1:  torch.Size([1, 16, 768])\n",
            "timestep:  8  inside train function1:  torch.Size([1, 16, 768])\n",
            "timestep:  9  inside train function1:  torch.Size([1, 16, 768])\n",
            "before calling backward pass\n",
            "inside trainIters after calling train\n",
            "Epoch:  1  Iteration:  1  avg_loss:  30.93995115492079\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFzGr5EK1wee"
      },
      "source": [
        "'''\n",
        "greedy search decoder\n",
        "'''\n",
        "def greedy_decode(decoder1,decoder2, decoder3, decoder_hidden, encoder_outputs, max_seq_len):\n",
        "        '''\n",
        "        :param target_tensor: target indexes tensor of shape [B, T] where B is the batch size and T is the maximum length of the output sentence\n",
        "        :param decoder_hidden: input tensor of shape [1, B, H] for start of the decoding\n",
        "        :param encoder_outputs: if you are using attention mechanism you can pass encoder outputs, [T, B, H] where T is the maximum length of input sentence\n",
        "        :return: decoded_batch\n",
        "        '''\n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "\n",
        "        ##decoded_batch = torch.zeros((batch_size, max_seq_len)).int()\n",
        "        decoded_batch_1 = torch.zeros((batch_size, max_seq_len)).int()\n",
        "        decoded_batch_2 = torch.zeros((batch_size, max_seq_len)).int()\n",
        "        decoded_batch_3 = torch.zeros((batch_size, max_seq_len)).int()\n",
        "        #print((decoded_batch.dtype))\n",
        "        # decoder_input = torch.LongTensor([[EN.vocab.stoi['<sos>']] for _ in range(batch_size)]).cuda()\n",
        "        #decoder_input = Variable(trg.data[0, :])  # sos\n",
        "        decoder_input = get_initial_decoder_ip(batch_size)\n",
        "        decoder1_input = decoder2_input =  decoder3_input = decoder_input\n",
        "        decoder1_hidden = decoder2_hidden = decoder3_hidden = decoder_hidden\n",
        "        print('1. inside greedy_decode decoder_input shape : ',decoder1_input.shape)\n",
        "        for timestep in range(max_seq_len):\n",
        "            ##decoder_output, decoder_hidden= decoder1(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder1_output, decoder1_hidden= decoder2(decoder1_input, decoder1_hidden, encoder_outputs)\n",
        "            decoder2_output, decoder2_hidden= decoder3(decoder2_input, decoder2_hidden, encoder_outputs)\n",
        "            decoder3_output, decoder3_hidden= decoder3(decoder3_input, decoder3_hidden, encoder_outputs)\n",
        "            print('2. inside greedy_decode decoder_output shape : ',decoder1_output.shape)# [1, 30521]\n",
        "            print('3. inside greedy_decode decoder_hidden shape : ',decoder1_hidden.shape)\n",
        "            ##topv, topi = decoder_output.data.topk(1)  \n",
        "            d1_topv, d1_topi = decoder1_output.data.topk(1)  \n",
        "            d2_topv, d2_topi = decoder2_output.data.topk(1)  \n",
        "            d3_topv, d3_topi = decoder3_output.data.topk(1)  \n",
        "            print('topv, topi: ',(d1_topv, d1_topi))\n",
        "            ##topi = topi.view(-1)\n",
        "            d1_topi =d1_topi.view(-1)\n",
        "            d2_topi = d2_topi.view(-1)\n",
        "            d3_topi = d3_topi.view(-1)\n",
        "            ##decoded_batch[:, timestep] = topi   \n",
        "            decoded_batch_1[:, timestep] = d1_topi   \n",
        "            decoded_batch_2[:, timestep] = d2_topi   \n",
        "            decoded_batch_3[:, timestep] = d3_topi   \n",
        "            ##attention_mask_decoded_batch = torch.where(decoded_batch != 0, 1, 0)        \n",
        "            d1_attention_mask_decoded_batch = torch.where(decoded_batch_1 != 0, 1, 0)   \n",
        "            d2_attention_mask_decoded_batch = torch.where(decoded_batch_2 != 0, 1, 0)   \n",
        "            d3_attention_mask_decoded_batch = torch.where(decoded_batch_3 != 0, 1, 0)   \n",
        "\n",
        "            ##decoder_input = topi.detach().view(-1)\n",
        "            decoder1_input = d1_topi.detach().view(-1)\n",
        "            decoder2_input = d2_topi.detach().view(-1)\n",
        "            decoder3_input = d3_topi.detach().view(-1)\n",
        "            print('4 inside greedy_decode decoder_input 2 : ',decoder1_input)\n",
        "            ##decoder_input = torch.unsqueeze(decoder_input, 0)\n",
        "            decoder1_input = torch.unsqueeze(decoder1_input, 0)\n",
        "            decoder2_input = torch.unsqueeze(decoder2_input, 0)\n",
        "            decoder3_input = torch.unsqueeze(decoder3_input, 0)\n",
        "            print('5 inside greedy_decode decoder_input 3 : ',decoder1_input)\n",
        "            with torch.no_grad():\n",
        "              ##last_hidden_states = model(decoded_batch, attention_mask=attention_mask_decoded_batch)\n",
        "              d1_last_hidden_states = model(decoded_batch_1, attention_mask=d1_attention_mask_decoded_batch)\n",
        "              d2_last_hidden_states = model(decoded_batch_2, attention_mask=d2_attention_mask_decoded_batch)\n",
        "              d3_last_hidden_states = model(decoded_batch_3, attention_mask=d3_attention_mask_decoded_batch)\n",
        "            ##decoded_batch_vector=last_hidden_states[0]#(batch_size*max_op_len*768)\n",
        "            d1_decoded_batch_vector=d1_last_hidden_states[0]#(batch_size*max_op_len*768)\n",
        "            d2_decoded_batch_vector=d2_last_hidden_states[0]#(batch_size*max_op_len*768)\n",
        "            d3_decoded_batch_vector=d3_last_hidden_states[0]#(batch_size*max_op_len*768)\n",
        "\n",
        "            ##decoder_input=torch.unsqueeze(decoded_batch_vector.permute(1,0,2)[timestep],0)\n",
        "            decoder1_input=torch.unsqueeze(d1_decoded_batch_vector.permute(1,0,2)[timestep],0)\n",
        "            decoder2_input=torch.unsqueeze(d2_decoded_batch_vector.permute(1,0,2)[timestep],0)\n",
        "            decoder3_input=torch.unsqueeze(d3_decoded_batch_vector.permute(1,0,2)[timestep],0)\n",
        "            print('6 inside greedy_decode decoder_input 5 : ',decoder1_input.shape)\n",
        "        return (decoded_batch_1,decoded_batch_2,decoded_batch_3)"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bo0pA_KwS1Y8"
      },
      "source": [
        ""
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNpn94M8YG8e"
      },
      "source": [
        "'''\n",
        "function for generating op sequence for a given input seq\n",
        "'''\n",
        "MAX_LENGTH=10\n",
        "def evaluate(summary_batch_vector, summary_length_batch, encoder, decoder1, decoder2, decoder3, max_length=MAX_LENGTH):\n",
        "  input_variable=summary_batch_vector.permute(1,0,2)\n",
        "  lengths = torch.tensor(summary_length_batch)\n",
        "  encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
        "  encoder_hidden = encoder_hidden[:decoder1.num_layers] \n",
        "  print('*************encoder forward pass complted***************')\n",
        "  decoded_batch_1,decoded_batch_2,decoded_batch_3=greedy_decode(decoder1, decoder2, decoder3, encoder_hidden, encoder_outputs, max_length)\n",
        "\n",
        "  d1_decoded_words = [tokenizer.decode(token_idx) for token_idx in decoded_batch_1[0]]\n",
        "  d2_decoded_words = [tokenizer.decode(token_idx) for token_idx in decoded_batch_2[0]]\n",
        "  d3_decoded_words = [tokenizer.decode(token_idx) for token_idx in decoded_batch_3[0]]\n",
        "  print('d1_decoded_words: ',d1_decoded_words)\n",
        "  print('d2_decoded_words: ',d2_decoded_words)\n",
        "  print('d3_decoded_words: ',d3_decoded_words)\n",
        "  return (d1_decoded_words,d2_decoded_words,d3_decoded_words)"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rt1G4-7F1wVO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98c893b8-d3f8-4822-ba8d-ced7b1818889"
      },
      "source": [
        "'''\n",
        "generate sequence for a given input news summary using 'greedy-search' \n",
        "'''\n",
        "news_summary = contents[0]\n",
        "ref_headline1 = target_headlines_1[0]\n",
        "ref_headline2 = target_headlines_2[0]\n",
        "ref_headline3 = target_headlines_3[0]\n",
        "\n",
        "print(\"article content: \")\n",
        "print(news_summary)\n",
        "print(\"reference headline: \",)\n",
        "print(ref_headline1)\n",
        "print(ref_headline2)\n",
        "print(ref_headline3)\n",
        "\n",
        "test_contents=contents[0:5]\n",
        "test_target_headlines_1=target_headlines_1[0:5]\n",
        "test_target_headlines_2=target_headlines_2[0:5]\n",
        "test_target_headlines_3=target_headlines_3[0:5]\n",
        "\n",
        "tokenized_test_summaries = [tokenizer(summary, padding=\"max_length\", truncation=True, max_length=max_encoder_len) for summary in test_contents]\n",
        "\n",
        "\n",
        "summary_batch = np.array([list(tokenized_test_summary.input_ids) for tokenized_test_summary in tokenized_test_summaries])\n",
        "summary_attention_mask = np.array([np.array(tokenized_test_summary.attention_mask) for tokenized_test_summary in tokenized_test_summaries])\n",
        "\n",
        "summary_length_batch=[np.count_nonzero(summary) for summary in summary_batch]\n",
        "\n",
        "#sort news summary based their no of tokens\n",
        "sorted_idx_pos=list(np.argsort(summary_length_batch)) #index position of input seq length based ascending order\n",
        "sorted_idx_pos.reverse() #to get descending order\n",
        "\n",
        "summary_length_batch.sort(reverse=True)\n",
        "summary_batch = rearrange_numpy_array(summary_batch,sorted_idx_pos)\n",
        "summary_attention_mask = rearrange_numpy_array(summary_attention_mask,sorted_idx_pos)\n",
        "\n",
        "'''\n",
        "tokenized_test_headlines_1 = [tokenizer(target_headlines, padding=\"max_length\", truncation=True, max_length=max_decoder_len) for target_headlines in test_target_headlines_1]\n",
        "tokenized_test_headlines_2 = [tokenizer(target_headlines, padding=\"max_length\", truncation=True, max_length=max_decoder_len) for target_headlines in test_target_headlines_2]\n",
        "tokenized_test_headlines_3 = [tokenizer(target_headlines, padding=\"max_length\", truncation=True, max_length=max_decoder_len) for target_headlines in test_target_headlines_3]\n",
        "\n",
        "test_outputs1_input_ids = np.array([np.array(tokenized_headline.input_ids) for tokenized_headline in tokenized_test_headlines_1])\n",
        "test_outputs1_attention_mask = np.array([np.array(tokenized_headline.attention_mask) for tokenized_headline in tokenized_test_headlines_1])\n",
        "test_outputs1_input_ids=rearrange_numpy_array(test_outputs1_input_ids,sorted_idx_pos)\n",
        "test_outputs1_attention_mask=rearrange_numpy_array(test_outputs1_attention_mask,sorted_idx_pos)\n",
        "\n",
        "test_outputs2_input_ids = np.array([np.array(tokenized_headline.input_ids) for tokenized_headline in tokenized_test_headlines_2])\n",
        "test_outputs2_attention_mask = np.array([np.array(tokenized_headline.attention_mask) for tokenized_headline in tokenized_test_headlines_2])\n",
        "test_outputs2_input_ids=rearrange_numpy_array(test_outputs2_input_ids,sorted_idx_pos)\n",
        "test_outputs2_attention_mask=rearrange_numpy_array(test_outputs2_attention_mask,sorted_idx_pos)\n",
        "\n",
        "test_outputs3_input_ids = np.array([np.array(tokenized_headline.input_ids) for tokenized_headline in tokenized_test_headlines_3])\n",
        "test_outputs3_attention_mask = np.array([np.array(tokenized_headline.attention_mask) for tokenized_headline in tokenized_test_headlines_3])\n",
        "test_outputs3_input_ids=rearrange_numpy_array(test_outputs3_input_ids,sorted_idx_pos)\n",
        "test_outputs3_attention_mask=rearrange_numpy_array(test_outputs3_attention_mask,sorted_idx_pos)\n",
        "'''\n",
        "###########\n",
        "\n",
        "summary_batch_t = torch.tensor(summary_batch) \n",
        "summary_attention_mask_t = torch.tensor(summary_attention_mask)\n",
        "\n",
        "with torch.no_grad():\n",
        "  last_hidden_states = model(summary_batch_t, attention_mask=summary_attention_mask_t)\n",
        "summary_batch_vector=last_hidden_states[0]\n",
        "\n",
        "print('summary_batch_vector shape: ',summary_batch_vector.shape)\n",
        "print('lengths: ',summary_length_batch)\n",
        "\n",
        "'''\n",
        "summary_batch=summary_batch.permute(1,0,2)\n",
        "summary_length_batch = torch.tensor(summary_length_batch)\n",
        "target_variable_1=target_variable_1.permute(1,0,2)\n",
        "target_variable_2=target_variable_2.permute(1,0,2)\n",
        "target_variable_3=target_variable_3.permute(1,0,2)\n",
        "print('target_variable shape: ',(target_variable_1.shape, target_variable_2.shape, target_variable_3.shape))\n",
        "'''\n",
        "MAX_LENGTH=10 # max target length of generated target seq\n",
        "evaluate(summary_batch_vector, summary_length_batch, encoder, decoder_1, decoder_2, decoder_3,MAX_LENGTH)\n",
        "#\n",
        "\n"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "article content: \n",
            "Taking to Instagram on Saturday, Arjun Kapoor posted a picture of himself with Janhvi Kapoor to wish the actress on her 24th birthday. In the picture, Arjun can be seen walking ahead while holding his sister's hand. \"Happy birthday Janhvi...I can't promise much except like this picture you shall always have my support & hand wherever you go,\" Arjun wrote.\n",
            "reference headline: \n",
            "You shall always have my support: Arjun Kapoor on Janhvi's b'day\n",
            "'You shall always have my support': Arjun Kapoor pens heart-warming birthday note for Janhvi\n",
            "\"You will always have my support,\" Arjun Kapoor writes a heartfelt birthday note for Janhvi Kapoor.\n",
            "summary_batch_vector shape:  torch.Size([5, 125, 768])\n",
            "lengths:  [93, 84, 83, 79, 76]\n",
            "inside encoder forward function || input_seq shape:  torch.Size([125, 5, 768])\n",
            "inside encoder forward function || input_lengths shape:  torch.Size([5])\n",
            "inside encoder forward function || outputs shape:  torch.Size([125, 5, 256])\n",
            "inside encoder forward function || hidden shape:  torch.Size([1, 5, 256])\n",
            "*************encoder forward pass complted***************\n",
            "decoder_input shape:  torch.Size([5, 1, 768])\n",
            "decoder_input shape:  torch.Size([1, 5, 768])\n",
            "1. inside greedy_decode decoder_input shape :  torch.Size([1, 5, 768])\n",
            "2. inside greedy_decode decoder_output shape :  torch.Size([5, 30521])\n",
            "3. inside greedy_decode decoder_hidden shape :  torch.Size([1, 5, 256])\n",
            "topv, topi:  (tensor([[4.9122e-05],\n",
            "        [5.4285e-05],\n",
            "        [5.1013e-05],\n",
            "        [5.1017e-05],\n",
            "        [4.9421e-05]]), tensor([[24222],\n",
            "        [23593],\n",
            "        [ 1230],\n",
            "        [ 1764],\n",
            "        [23593]]))\n",
            "4 inside greedy_decode decoder_input 2 :  tensor([24222, 23593,  1230,  1764, 23593])\n",
            "5 inside greedy_decode decoder_input 3 :  tensor([[24222, 23593,  1230,  1764, 23593]])\n",
            "6 inside greedy_decode decoder_input 5 :  torch.Size([1, 5, 768])\n",
            "2. inside greedy_decode decoder_output shape :  torch.Size([5, 30521])\n",
            "3. inside greedy_decode decoder_hidden shape :  torch.Size([1, 5, 256])\n",
            "topv, topi:  (tensor([[4.8351e-05],\n",
            "        [5.1057e-05],\n",
            "        [5.2657e-05],\n",
            "        [5.0975e-05],\n",
            "        [5.0287e-05]]), tensor([[27791],\n",
            "        [ 1230],\n",
            "        [23593],\n",
            "        [ 1764],\n",
            "        [ 5404]]))\n",
            "4 inside greedy_decode decoder_input 2 :  tensor([27791,  1230, 23593,  1764,  5404])\n",
            "5 inside greedy_decode decoder_input 3 :  tensor([[27791,  1230, 23593,  1764,  5404]])\n",
            "6 inside greedy_decode decoder_input 5 :  torch.Size([1, 5, 768])\n",
            "2. inside greedy_decode decoder_output shape :  torch.Size([5, 30521])\n",
            "3. inside greedy_decode decoder_hidden shape :  torch.Size([1, 5, 256])\n",
            "topv, topi:  (tensor([[4.9738e-05],\n",
            "        [5.3008e-05],\n",
            "        [5.0254e-05],\n",
            "        [5.3081e-05],\n",
            "        [5.1243e-05]]), tensor([[14632],\n",
            "        [26595],\n",
            "        [ 3799],\n",
            "        [27791],\n",
            "        [ 1256]]))\n",
            "4 inside greedy_decode decoder_input 2 :  tensor([14632, 26595,  3799, 27791,  1256])\n",
            "5 inside greedy_decode decoder_input 3 :  tensor([[14632, 26595,  3799, 27791,  1256]])\n",
            "6 inside greedy_decode decoder_input 5 :  torch.Size([1, 5, 768])\n",
            "2. inside greedy_decode decoder_output shape :  torch.Size([5, 30521])\n",
            "3. inside greedy_decode decoder_hidden shape :  torch.Size([1, 5, 256])\n",
            "topv, topi:  (tensor([[5.2821e-05],\n",
            "        [5.7706e-05],\n",
            "        [5.1195e-05],\n",
            "        [5.0600e-05],\n",
            "        [5.0293e-05]]), tensor([[27791],\n",
            "        [26595],\n",
            "        [26595],\n",
            "        [ 1764],\n",
            "        [23040]]))\n",
            "4 inside greedy_decode decoder_input 2 :  tensor([27791, 26595, 26595,  1764, 23040])\n",
            "5 inside greedy_decode decoder_input 3 :  tensor([[27791, 26595, 26595,  1764, 23040]])\n",
            "6 inside greedy_decode decoder_input 5 :  torch.Size([1, 5, 768])\n",
            "2. inside greedy_decode decoder_output shape :  torch.Size([5, 30521])\n",
            "3. inside greedy_decode decoder_hidden shape :  torch.Size([1, 5, 256])\n",
            "topv, topi:  (tensor([[5.2919e-05],\n",
            "        [5.8289e-05],\n",
            "        [5.1332e-05],\n",
            "        [5.1172e-05],\n",
            "        [4.9073e-05]]), tensor([[27791],\n",
            "        [26595],\n",
            "        [26487],\n",
            "        [ 1764],\n",
            "        [22421]]))\n",
            "4 inside greedy_decode decoder_input 2 :  tensor([27791, 26595, 26487,  1764, 22421])\n",
            "5 inside greedy_decode decoder_input 3 :  tensor([[27791, 26595, 26487,  1764, 22421]])\n",
            "6 inside greedy_decode decoder_input 5 :  torch.Size([1, 5, 768])\n",
            "2. inside greedy_decode decoder_output shape :  torch.Size([5, 30521])\n",
            "3. inside greedy_decode decoder_hidden shape :  torch.Size([1, 5, 256])\n",
            "topv, topi:  (tensor([[5.3079e-05],\n",
            "        [5.7829e-05],\n",
            "        [5.4898e-05],\n",
            "        [5.3184e-05],\n",
            "        [4.8961e-05]]), tensor([[27791],\n",
            "        [26595],\n",
            "        [23878],\n",
            "        [23945],\n",
            "        [22421]]))\n",
            "4 inside greedy_decode decoder_input 2 :  tensor([27791, 26595, 23878, 23945, 22421])\n",
            "5 inside greedy_decode decoder_input 3 :  tensor([[27791, 26595, 23878, 23945, 22421]])\n",
            "6 inside greedy_decode decoder_input 5 :  torch.Size([1, 5, 768])\n",
            "2. inside greedy_decode decoder_output shape :  torch.Size([5, 30521])\n",
            "3. inside greedy_decode decoder_hidden shape :  torch.Size([1, 5, 256])\n",
            "topv, topi:  (tensor([[5.2442e-05],\n",
            "        [5.7308e-05],\n",
            "        [5.3632e-05],\n",
            "        [5.2222e-05],\n",
            "        [4.9598e-05]]), tensor([[27791],\n",
            "        [26595],\n",
            "        [26487],\n",
            "        [23945],\n",
            "        [22421]]))\n",
            "4 inside greedy_decode decoder_input 2 :  tensor([27791, 26595, 26487, 23945, 22421])\n",
            "5 inside greedy_decode decoder_input 3 :  tensor([[27791, 26595, 26487, 23945, 22421]])\n",
            "6 inside greedy_decode decoder_input 5 :  torch.Size([1, 5, 768])\n",
            "2. inside greedy_decode decoder_output shape :  torch.Size([5, 30521])\n",
            "3. inside greedy_decode decoder_hidden shape :  torch.Size([1, 5, 256])\n",
            "topv, topi:  (tensor([[5.1537e-05],\n",
            "        [5.6874e-05],\n",
            "        [5.3612e-05],\n",
            "        [5.1454e-05],\n",
            "        [5.0122e-05]]), tensor([[27791],\n",
            "        [26595],\n",
            "        [23878],\n",
            "        [23945],\n",
            "        [22421]]))\n",
            "4 inside greedy_decode decoder_input 2 :  tensor([27791, 26595, 23878, 23945, 22421])\n",
            "5 inside greedy_decode decoder_input 3 :  tensor([[27791, 26595, 23878, 23945, 22421]])\n",
            "6 inside greedy_decode decoder_input 5 :  torch.Size([1, 5, 768])\n",
            "2. inside greedy_decode decoder_output shape :  torch.Size([5, 30521])\n",
            "3. inside greedy_decode decoder_hidden shape :  torch.Size([1, 5, 256])\n",
            "topv, topi:  (tensor([[5.2649e-05],\n",
            "        [5.6583e-05],\n",
            "        [5.1976e-05],\n",
            "        [5.1272e-05],\n",
            "        [4.9987e-05]]), tensor([[ 5381],\n",
            "        [26595],\n",
            "        [23878],\n",
            "        [24226],\n",
            "        [22421]]))\n",
            "4 inside greedy_decode decoder_input 2 :  tensor([ 5381, 26595, 23878, 24226, 22421])\n",
            "5 inside greedy_decode decoder_input 3 :  tensor([[ 5381, 26595, 23878, 24226, 22421]])\n",
            "6 inside greedy_decode decoder_input 5 :  torch.Size([1, 5, 768])\n",
            "2. inside greedy_decode decoder_output shape :  torch.Size([5, 30521])\n",
            "3. inside greedy_decode decoder_hidden shape :  torch.Size([1, 5, 256])\n",
            "topv, topi:  (tensor([[5.2513e-05],\n",
            "        [5.6313e-05],\n",
            "        [5.2342e-05],\n",
            "        [5.0647e-05],\n",
            "        [4.9899e-05]]), tensor([[ 5381],\n",
            "        [26595],\n",
            "        [14632],\n",
            "        [27791],\n",
            "        [21425]]))\n",
            "4 inside greedy_decode decoder_input 2 :  tensor([ 5381, 26595, 14632, 27791, 21425])\n",
            "5 inside greedy_decode decoder_input 3 :  tensor([[ 5381, 26595, 14632, 27791, 21425]])\n",
            "6 inside greedy_decode decoder_input 5 :  torch.Size([1, 5, 768])\n",
            "d1_decoded_words:  ['c h i l l y', 'c o m m i t s', 'm a n n e r s', 'c o m m i t s', 'c o m m i t s', 'c o m m i t s', 'c o m m i t s', 'c o m m i t s', 'p e r s o n s', 'p e r s o n s']\n",
            "d2_decoded_words:  ['# # a t i o n s', 'd o m', 'd o m', '☆', 'd o m', 'p d f', 'p d f', '# # s a u', 'i n t e l l e c t', 'i n t e l l e c t']\n",
            "d3_decoded_words:  ['# # a t i o n s', 'd o m', 'd o m', '☆', 'd o m', 'p d f', 'p d f', '# # s a u', 'i n t e l l e c t', 'i n t e l l e c t']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['c h i l l y',\n",
              "  'c o m m i t s',\n",
              "  'm a n n e r s',\n",
              "  'c o m m i t s',\n",
              "  'c o m m i t s',\n",
              "  'c o m m i t s',\n",
              "  'c o m m i t s',\n",
              "  'c o m m i t s',\n",
              "  'p e r s o n s',\n",
              "  'p e r s o n s'],\n",
              " ['# # a t i o n s',\n",
              "  'd o m',\n",
              "  'd o m',\n",
              "  '☆',\n",
              "  'd o m',\n",
              "  'p d f',\n",
              "  'p d f',\n",
              "  '# # s a u',\n",
              "  'i n t e l l e c t',\n",
              "  'i n t e l l e c t'],\n",
              " ['# # a t i o n s',\n",
              "  'd o m',\n",
              "  'd o m',\n",
              "  '☆',\n",
              "  'd o m',\n",
              "  'p d f',\n",
              "  'p d f',\n",
              "  '# # s a u',\n",
              "  'i n t e l l e c t',\n",
              "  'i n t e l l e c t'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqTbBA7f1wR1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKpCO1471wOn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-QwJAKfRLLz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}